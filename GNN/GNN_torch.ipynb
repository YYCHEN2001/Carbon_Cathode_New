{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:43:53.588498Z",
     "start_time": "2024-11-26T09:43:52.270138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "e7d30d5c108ee240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:43:55.483026Z",
     "start_time": "2024-11-26T09:43:53.592500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"../data/dataset.csv\")\n",
    "data['target_class'] = pd.qcut(data['Cs'], q=10, labels=False)\n",
    "X = data.drop(['Cs', 'target_class'], axis=1)\n",
    "y = data['Cs']\n",
    "stratify_column = data['target_class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=stratify_column)\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the number of material features and test conditions\n",
    "num_material_features = 7\n",
    "num_test_conditions = 5\n",
    "num_features = num_material_features + num_test_conditions\n",
    "\n",
    "# Construct edges: connect each node to its immediate neighbors\n",
    "# edges = []\n",
    "# for i in range(num_material_features):\n",
    "#     if i < num_material_features - 1:\n",
    "#         edges.append([i, i + 1])\n",
    "#         edges.append([i + 1, i])\n",
    "\n",
    "\n",
    "edges = [[0,1],[1,0],[1,2],[1,6],[2,1],[2,3],[3,2],[3,4],[4,3],[4,5],[4,6],[5,4],[6,1],[6,4]]\n",
    "print(edges)\n",
    "\n",
    "# 转换为Tensor\n",
    "edges = np.array(edges).T  # 转置以匹配PyTorch Geometric的edge_index格式\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1) # 确保y是列向量\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)  # 确保y是列向量\n",
    "\n",
    "train_data = Data(x=X_train_tensor, edge_index=edge_index, y=y_train_tensor).to(device)\n",
    "test_data = Data(x=X_test_tensor, edge_index=edge_index, y=y_test_tensor).to(device)\n",
    "\n",
    "train_loader = DataLoader([train_data], batch_size=20, shuffle=True)  # 根据实际数据调整\n",
    "test_loader = DataLoader([test_data], batch_size=20, shuffle=False)  # 根据实际数据调整\n",
    "print(edges)\n",
    "print(edge_index)"
   ],
   "id": "b295e86f637a2b4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [1, 0], [1, 2], [1, 6], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [4, 6], [5, 4], [6, 1], [6, 4]]\n",
      "[[0 1 1 1 2 2 3 3 4 4 4 5 6 6]\n",
      " [1 0 2 6 1 3 2 4 3 5 6 4 1 4]]\n",
      "tensor([[0, 1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 5, 6, 6],\n",
      "        [1, 0, 2, 6, 1, 3, 2, 4, 3, 5, 6, 4, 1, 4]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:44:01.526524Z",
     "start_time": "2024-11-26T09:43:55.561013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from GNN_torch import GNNModel\n",
    "from torch_loss import RMSE_Loss\n",
    "\n",
    "model = GNNModel(input_dim=12, hidden_dims=[64,64], output_dim=1)  # 根据实际输入维度调整\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "loss_fn = nn.MSELoss().to(device)\n",
    "\n",
    "num_epochs = 3000\n",
    "best_loss = float('inf')\n",
    "cumulative_loss = 0.0\n",
    "patience = 20  # 允许的最大连续未改进 epoch 数\n",
    "epochs_without_improvement = 0  # 连续未改进的 epoch 数\n",
    "model.train()\n",
    "\n",
    "# 训练过程\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    cumulative_loss = 0.0  # 每个epoch重新初始化累积损失\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_fn(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cumulative_loss += loss.item()\n",
    "\n",
    "    # 每10个epoch输出一次平均损失\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        average_loss = cumulative_loss / 10\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {average_loss:.4f}')\n",
    "        cumulative_loss = 0.0\n",
    "\n",
    "    # 计算验证集的损失\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            loss = loss_fn(out, data.y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), \"gnn_best_model.pth\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # 如果连续`patience`个epoch未改进，进行早停\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break"
   ],
   "id": "18cb525202dbc92f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 10302.7900\n",
      "Epoch 2, Validation Loss: 10265.8047\n",
      "Epoch 3, Validation Loss: 10228.3096\n",
      "Epoch 4, Validation Loss: 10190.1260\n",
      "Epoch 5, Validation Loss: 10150.6309\n",
      "Epoch 6, Validation Loss: 10110.0381\n",
      "Epoch 7, Validation Loss: 10068.2373\n",
      "Epoch 8, Validation Loss: 10025.1816\n",
      "Epoch 9, Validation Loss: 9980.2715\n",
      "Epoch 10, Average Loss: 995.2084\n",
      "Epoch 10, Validation Loss: 9933.1514\n",
      "Epoch 11, Validation Loss: 9883.8975\n",
      "Epoch 12, Validation Loss: 9832.1045\n",
      "Epoch 13, Validation Loss: 9777.1006\n",
      "Epoch 14, Validation Loss: 9719.1182\n",
      "Epoch 15, Validation Loss: 9658.1221\n",
      "Epoch 16, Validation Loss: 9593.9482\n",
      "Epoch 17, Validation Loss: 9526.5898\n",
      "Epoch 18, Validation Loss: 9455.8086\n",
      "Epoch 19, Validation Loss: 9381.3711\n",
      "Epoch 20, Average Loss: 936.7913\n",
      "Epoch 20, Validation Loss: 9303.1904\n",
      "Epoch 21, Validation Loss: 9221.0605\n",
      "Epoch 22, Validation Loss: 9134.8896\n",
      "Epoch 23, Validation Loss: 9044.6846\n",
      "Epoch 24, Validation Loss: 8950.4375\n",
      "Epoch 25, Validation Loss: 8852.0088\n",
      "Epoch 26, Validation Loss: 8749.1230\n",
      "Epoch 27, Validation Loss: 8641.7568\n",
      "Epoch 28, Validation Loss: 8529.9990\n",
      "Epoch 29, Validation Loss: 8413.8408\n",
      "Epoch 30, Average Loss: 842.1786\n",
      "Epoch 30, Validation Loss: 8293.3125\n",
      "Epoch 31, Validation Loss: 8168.3818\n",
      "Epoch 32, Validation Loss: 8039.0923\n",
      "Epoch 33, Validation Loss: 7905.3965\n",
      "Epoch 34, Validation Loss: 7767.2603\n",
      "Epoch 35, Validation Loss: 7624.7729\n",
      "Epoch 36, Validation Loss: 7478.0366\n",
      "Epoch 37, Validation Loss: 7327.1553\n",
      "Epoch 38, Validation Loss: 7172.1885\n",
      "Epoch 39, Validation Loss: 7013.2520\n",
      "Epoch 40, Average Loss: 705.0321\n",
      "Epoch 40, Validation Loss: 6850.5044\n",
      "Epoch 41, Validation Loss: 6684.0254\n",
      "Epoch 42, Validation Loss: 6514.0430\n",
      "Epoch 43, Validation Loss: 6340.8140\n",
      "Epoch 44, Validation Loss: 6164.6387\n",
      "Epoch 45, Validation Loss: 5985.8408\n",
      "Epoch 46, Validation Loss: 5804.7637\n",
      "Epoch 47, Validation Loss: 5621.6724\n",
      "Epoch 48, Validation Loss: 5436.9399\n",
      "Epoch 49, Validation Loss: 5251.0449\n",
      "Epoch 50, Average Loss: 531.6361\n",
      "Epoch 50, Validation Loss: 5064.4551\n",
      "Epoch 51, Validation Loss: 4877.6880\n",
      "Epoch 52, Validation Loss: 4691.1958\n",
      "Epoch 53, Validation Loss: 4505.3950\n",
      "Epoch 54, Validation Loss: 4320.8979\n",
      "Epoch 55, Validation Loss: 4138.2354\n",
      "Epoch 56, Validation Loss: 3957.9878\n",
      "Epoch 57, Validation Loss: 3780.7517\n",
      "Epoch 58, Validation Loss: 3607.0603\n",
      "Epoch 59, Validation Loss: 3437.4656\n",
      "Epoch 60, Average Loss: 351.4348\n",
      "Epoch 60, Validation Loss: 3272.4622\n",
      "Epoch 61, Validation Loss: 3112.5281\n",
      "Epoch 62, Validation Loss: 2958.1602\n",
      "Epoch 63, Validation Loss: 2809.8108\n",
      "Epoch 64, Validation Loss: 2667.8972\n",
      "Epoch 65, Validation Loss: 2532.7310\n",
      "Epoch 66, Validation Loss: 2404.6323\n",
      "Epoch 67, Validation Loss: 2283.8123\n",
      "Epoch 68, Validation Loss: 2170.5515\n",
      "Epoch 69, Validation Loss: 2065.0044\n",
      "Epoch 70, Average Loss: 213.0695\n",
      "Epoch 70, Validation Loss: 1967.3252\n",
      "Epoch 71, Validation Loss: 1877.4698\n",
      "Epoch 72, Validation Loss: 1795.3262\n",
      "Epoch 73, Validation Loss: 1720.7791\n",
      "Epoch 74, Validation Loss: 1653.8439\n",
      "Epoch 75, Validation Loss: 1594.0637\n",
      "Epoch 76, Validation Loss: 1541.0587\n",
      "Epoch 77, Validation Loss: 1494.4218\n",
      "Epoch 78, Validation Loss: 1453.7183\n",
      "Epoch 79, Validation Loss: 1418.4012\n",
      "Epoch 80, Average Loss: 147.5571\n",
      "Epoch 80, Validation Loss: 1387.9382\n",
      "Epoch 81, Validation Loss: 1361.8003\n",
      "Epoch 82, Validation Loss: 1339.3014\n",
      "Epoch 83, Validation Loss: 1319.8784\n",
      "Epoch 84, Validation Loss: 1302.9438\n",
      "Epoch 85, Validation Loss: 1288.0702\n",
      "Epoch 86, Validation Loss: 1274.6873\n",
      "Epoch 87, Validation Loss: 1262.4152\n",
      "Epoch 88, Validation Loss: 1250.8939\n",
      "Epoch 89, Validation Loss: 1239.8542\n",
      "Epoch 90, Average Loss: 130.2383\n",
      "Epoch 90, Validation Loss: 1229.1204\n",
      "Epoch 91, Validation Loss: 1218.5592\n",
      "Epoch 92, Validation Loss: 1208.0548\n",
      "Epoch 93, Validation Loss: 1197.5631\n",
      "Epoch 94, Validation Loss: 1187.1277\n",
      "Epoch 95, Validation Loss: 1176.7927\n",
      "Epoch 96, Validation Loss: 1166.5847\n",
      "Epoch 97, Validation Loss: 1156.5229\n",
      "Epoch 98, Validation Loss: 1146.7037\n",
      "Epoch 99, Validation Loss: 1137.1093\n",
      "Epoch 100, Average Loss: 120.0631\n",
      "Epoch 100, Validation Loss: 1127.8688\n",
      "Epoch 101, Validation Loss: 1119.1320\n",
      "Epoch 102, Validation Loss: 1110.7485\n",
      "Epoch 103, Validation Loss: 1102.7684\n",
      "Epoch 104, Validation Loss: 1095.1421\n",
      "Epoch 105, Validation Loss: 1087.9060\n",
      "Epoch 106, Validation Loss: 1081.2096\n",
      "Epoch 107, Validation Loss: 1074.9529\n",
      "Epoch 108, Validation Loss: 1069.0625\n",
      "Epoch 109, Validation Loss: 1063.4841\n",
      "Epoch 110, Average Loss: 112.0316\n",
      "Epoch 110, Validation Loss: 1058.1478\n",
      "Epoch 111, Validation Loss: 1053.0579\n",
      "Epoch 112, Validation Loss: 1048.2126\n",
      "Epoch 113, Validation Loss: 1043.5590\n",
      "Epoch 114, Validation Loss: 1039.0693\n",
      "Epoch 115, Validation Loss: 1034.7074\n",
      "Epoch 116, Validation Loss: 1030.3839\n",
      "Epoch 117, Validation Loss: 1026.1334\n",
      "Epoch 118, Validation Loss: 1021.9657\n",
      "Epoch 119, Validation Loss: 1017.8849\n",
      "Epoch 120, Average Loss: 107.1131\n",
      "Epoch 120, Validation Loss: 1013.8484\n",
      "Epoch 121, Validation Loss: 1009.8657\n",
      "Epoch 122, Validation Loss: 1005.9487\n",
      "Epoch 123, Validation Loss: 1002.0914\n",
      "Epoch 124, Validation Loss: 998.3231\n",
      "Epoch 125, Validation Loss: 994.6147\n",
      "Epoch 126, Validation Loss: 991.0133\n",
      "Epoch 127, Validation Loss: 987.4821\n",
      "Epoch 128, Validation Loss: 984.0143\n",
      "Epoch 129, Validation Loss: 980.6353\n",
      "Epoch 130, Average Loss: 103.2332\n",
      "Epoch 130, Validation Loss: 977.3207\n",
      "Epoch 131, Validation Loss: 974.0599\n",
      "Epoch 132, Validation Loss: 970.8607\n",
      "Epoch 133, Validation Loss: 967.7452\n",
      "Epoch 134, Validation Loss: 964.6918\n",
      "Epoch 135, Validation Loss: 961.6993\n",
      "Epoch 136, Validation Loss: 958.7792\n",
      "Epoch 137, Validation Loss: 955.9108\n",
      "Epoch 138, Validation Loss: 953.0996\n",
      "Epoch 139, Validation Loss: 950.3317\n",
      "Epoch 140, Average Loss: 100.0202\n",
      "Epoch 140, Validation Loss: 947.6152\n",
      "Epoch 141, Validation Loss: 944.9388\n",
      "Epoch 142, Validation Loss: 942.3111\n",
      "Epoch 143, Validation Loss: 939.7046\n",
      "Epoch 144, Validation Loss: 937.1333\n",
      "Epoch 145, Validation Loss: 934.6059\n",
      "Epoch 146, Validation Loss: 932.1254\n",
      "Epoch 147, Validation Loss: 929.6903\n",
      "Epoch 148, Validation Loss: 927.3095\n",
      "Epoch 149, Validation Loss: 924.9540\n",
      "Epoch 150, Average Loss: 97.2621\n",
      "Epoch 150, Validation Loss: 922.6177\n",
      "Epoch 151, Validation Loss: 920.2938\n",
      "Epoch 152, Validation Loss: 917.9886\n",
      "Epoch 153, Validation Loss: 915.7072\n",
      "Epoch 154, Validation Loss: 913.4492\n",
      "Epoch 155, Validation Loss: 911.2061\n",
      "Epoch 156, Validation Loss: 908.9870\n",
      "Epoch 157, Validation Loss: 906.8104\n",
      "Epoch 158, Validation Loss: 904.6788\n",
      "Epoch 159, Validation Loss: 902.5587\n",
      "Epoch 160, Average Loss: 94.7152\n",
      "Epoch 160, Validation Loss: 900.4523\n",
      "Epoch 161, Validation Loss: 898.3762\n",
      "Epoch 162, Validation Loss: 896.3257\n",
      "Epoch 163, Validation Loss: 894.2873\n",
      "Epoch 164, Validation Loss: 892.2642\n",
      "Epoch 165, Validation Loss: 890.2765\n",
      "Epoch 166, Validation Loss: 888.3158\n",
      "Epoch 167, Validation Loss: 886.3780\n",
      "Epoch 168, Validation Loss: 884.4710\n",
      "Epoch 169, Validation Loss: 882.5901\n",
      "Epoch 170, Average Loss: 92.3705\n",
      "Epoch 170, Validation Loss: 880.7327\n",
      "Epoch 171, Validation Loss: 878.9050\n",
      "Epoch 172, Validation Loss: 877.0974\n",
      "Epoch 173, Validation Loss: 875.3193\n",
      "Epoch 174, Validation Loss: 873.5579\n",
      "Epoch 175, Validation Loss: 871.7974\n",
      "Epoch 176, Validation Loss: 870.0483\n",
      "Epoch 177, Validation Loss: 868.3048\n",
      "Epoch 178, Validation Loss: 866.5736\n",
      "Epoch 179, Validation Loss: 864.8557\n",
      "Epoch 180, Average Loss: 90.2800\n",
      "Epoch 180, Validation Loss: 863.1447\n",
      "Epoch 181, Validation Loss: 861.4286\n",
      "Epoch 182, Validation Loss: 859.7017\n",
      "Epoch 183, Validation Loss: 857.9762\n",
      "Epoch 184, Validation Loss: 856.2697\n",
      "Epoch 185, Validation Loss: 854.5764\n",
      "Epoch 186, Validation Loss: 852.9038\n",
      "Epoch 187, Validation Loss: 851.2648\n",
      "Epoch 188, Validation Loss: 849.6559\n",
      "Epoch 189, Validation Loss: 848.0925\n",
      "Epoch 190, Average Loss: 88.3408\n",
      "Epoch 190, Validation Loss: 846.5452\n",
      "Epoch 191, Validation Loss: 845.0286\n",
      "Epoch 192, Validation Loss: 843.5327\n",
      "Epoch 193, Validation Loss: 842.0527\n",
      "Epoch 194, Validation Loss: 840.5834\n",
      "Epoch 195, Validation Loss: 839.1216\n",
      "Epoch 196, Validation Loss: 837.6892\n",
      "Epoch 197, Validation Loss: 836.2697\n",
      "Epoch 198, Validation Loss: 834.8663\n",
      "Epoch 199, Validation Loss: 833.4848\n",
      "Epoch 200, Average Loss: 86.5967\n",
      "Epoch 200, Validation Loss: 832.1157\n",
      "Epoch 201, Validation Loss: 830.7595\n",
      "Epoch 202, Validation Loss: 829.4058\n",
      "Epoch 203, Validation Loss: 828.0504\n",
      "Epoch 204, Validation Loss: 826.7043\n",
      "Epoch 205, Validation Loss: 825.3745\n",
      "Epoch 206, Validation Loss: 824.0654\n",
      "Epoch 207, Validation Loss: 822.7595\n",
      "Epoch 208, Validation Loss: 821.4547\n",
      "Epoch 209, Validation Loss: 820.1486\n",
      "Epoch 210, Average Loss: 84.9924\n",
      "Epoch 210, Validation Loss: 818.8466\n",
      "Epoch 211, Validation Loss: 817.5586\n",
      "Epoch 212, Validation Loss: 816.2612\n",
      "Epoch 213, Validation Loss: 814.9826\n",
      "Epoch 214, Validation Loss: 813.7309\n",
      "Epoch 215, Validation Loss: 812.4865\n",
      "Epoch 216, Validation Loss: 811.2420\n",
      "Epoch 217, Validation Loss: 810.0177\n",
      "Epoch 218, Validation Loss: 808.8102\n",
      "Epoch 219, Validation Loss: 807.6174\n",
      "Epoch 220, Average Loss: 83.5372\n",
      "Epoch 220, Validation Loss: 806.4203\n",
      "Epoch 221, Validation Loss: 805.2313\n",
      "Epoch 222, Validation Loss: 804.0452\n",
      "Epoch 223, Validation Loss: 802.8762\n",
      "Epoch 224, Validation Loss: 801.7073\n",
      "Epoch 225, Validation Loss: 800.5538\n",
      "Epoch 226, Validation Loss: 799.4175\n",
      "Epoch 227, Validation Loss: 798.3006\n",
      "Epoch 228, Validation Loss: 797.1949\n",
      "Epoch 229, Validation Loss: 796.0924\n",
      "Epoch 230, Average Loss: 82.2205\n",
      "Epoch 230, Validation Loss: 794.9912\n",
      "Epoch 231, Validation Loss: 793.8870\n",
      "Epoch 232, Validation Loss: 792.7957\n",
      "Epoch 233, Validation Loss: 791.7211\n",
      "Epoch 234, Validation Loss: 790.6543\n",
      "Epoch 235, Validation Loss: 789.5992\n",
      "Epoch 236, Validation Loss: 788.5623\n",
      "Epoch 237, Validation Loss: 787.5364\n",
      "Epoch 238, Validation Loss: 786.5189\n",
      "Epoch 239, Validation Loss: 785.5043\n",
      "Epoch 240, Average Loss: 80.9910\n",
      "Epoch 240, Validation Loss: 784.4911\n",
      "Epoch 241, Validation Loss: 783.4924\n",
      "Epoch 242, Validation Loss: 782.5092\n",
      "Epoch 243, Validation Loss: 781.5248\n",
      "Epoch 244, Validation Loss: 780.5399\n",
      "Epoch 245, Validation Loss: 779.5644\n",
      "Epoch 246, Validation Loss: 778.5900\n",
      "Epoch 247, Validation Loss: 777.6170\n",
      "Epoch 248, Validation Loss: 776.6475\n",
      "Epoch 249, Validation Loss: 775.6791\n",
      "Epoch 250, Average Loss: 79.8217\n",
      "Epoch 250, Validation Loss: 774.7127\n",
      "Epoch 251, Validation Loss: 773.7570\n",
      "Epoch 252, Validation Loss: 772.8077\n",
      "Epoch 253, Validation Loss: 771.8637\n",
      "Epoch 254, Validation Loss: 770.9227\n",
      "Epoch 255, Validation Loss: 769.9907\n",
      "Epoch 256, Validation Loss: 769.0856\n",
      "Epoch 257, Validation Loss: 768.1958\n",
      "Epoch 258, Validation Loss: 767.3099\n",
      "Epoch 259, Validation Loss: 766.4324\n",
      "Epoch 260, Average Loss: 78.7077\n",
      "Epoch 260, Validation Loss: 765.5580\n",
      "Epoch 261, Validation Loss: 764.6843\n",
      "Epoch 262, Validation Loss: 763.8142\n",
      "Epoch 263, Validation Loss: 762.9448\n",
      "Epoch 264, Validation Loss: 762.0615\n",
      "Epoch 265, Validation Loss: 761.1506\n",
      "Epoch 266, Validation Loss: 760.2427\n",
      "Epoch 267, Validation Loss: 759.3475\n",
      "Epoch 268, Validation Loss: 758.4698\n",
      "Epoch 269, Validation Loss: 757.6110\n",
      "Epoch 270, Average Loss: 77.6465\n",
      "Epoch 270, Validation Loss: 756.7554\n",
      "Epoch 271, Validation Loss: 755.8916\n",
      "Epoch 272, Validation Loss: 755.0131\n",
      "Epoch 273, Validation Loss: 754.1230\n",
      "Epoch 274, Validation Loss: 753.2242\n",
      "Epoch 275, Validation Loss: 752.3081\n",
      "Epoch 276, Validation Loss: 751.4031\n",
      "Epoch 277, Validation Loss: 750.5225\n",
      "Epoch 278, Validation Loss: 749.6462\n",
      "Epoch 279, Validation Loss: 748.7719\n",
      "Epoch 280, Average Loss: 76.5841\n",
      "Epoch 280, Validation Loss: 747.9103\n",
      "Epoch 281, Validation Loss: 747.0416\n",
      "Epoch 282, Validation Loss: 746.1832\n",
      "Epoch 283, Validation Loss: 745.3287\n",
      "Epoch 284, Validation Loss: 744.4780\n",
      "Epoch 285, Validation Loss: 743.6367\n",
      "Epoch 286, Validation Loss: 742.8074\n",
      "Epoch 287, Validation Loss: 741.9844\n",
      "Epoch 288, Validation Loss: 741.1672\n",
      "Epoch 289, Validation Loss: 740.3727\n",
      "Epoch 290, Average Loss: 75.5395\n",
      "Epoch 290, Validation Loss: 739.5983\n",
      "Epoch 291, Validation Loss: 738.8278\n",
      "Epoch 292, Validation Loss: 738.0520\n",
      "Epoch 293, Validation Loss: 737.2759\n",
      "Epoch 294, Validation Loss: 736.4951\n",
      "Epoch 295, Validation Loss: 735.7113\n",
      "Epoch 296, Validation Loss: 734.9326\n",
      "Epoch 297, Validation Loss: 734.1503\n",
      "Epoch 298, Validation Loss: 733.3783\n",
      "Epoch 299, Validation Loss: 732.6115\n",
      "Epoch 300, Average Loss: 74.5912\n",
      "Epoch 300, Validation Loss: 731.8304\n",
      "Epoch 301, Validation Loss: 731.0574\n",
      "Epoch 302, Validation Loss: 730.2885\n",
      "Epoch 303, Validation Loss: 729.5262\n",
      "Epoch 304, Validation Loss: 728.7663\n",
      "Epoch 305, Validation Loss: 728.0031\n",
      "Epoch 306, Validation Loss: 727.2532\n",
      "Epoch 307, Validation Loss: 726.4958\n",
      "Epoch 308, Validation Loss: 725.7218\n",
      "Epoch 309, Validation Loss: 724.9608\n",
      "Epoch 310, Average Loss: 73.6956\n",
      "Epoch 310, Validation Loss: 724.2007\n",
      "Epoch 311, Validation Loss: 723.4495\n",
      "Epoch 312, Validation Loss: 722.7191\n",
      "Epoch 313, Validation Loss: 722.0103\n",
      "Epoch 314, Validation Loss: 721.3252\n",
      "Epoch 315, Validation Loss: 720.6470\n",
      "Epoch 316, Validation Loss: 719.9716\n",
      "Epoch 317, Validation Loss: 719.3087\n",
      "Epoch 318, Validation Loss: 718.6542\n",
      "Epoch 319, Validation Loss: 718.0056\n",
      "Epoch 320, Average Loss: 72.8590\n",
      "Epoch 320, Validation Loss: 717.3492\n",
      "Epoch 321, Validation Loss: 716.6865\n",
      "Epoch 322, Validation Loss: 716.0327\n",
      "Epoch 323, Validation Loss: 715.3792\n",
      "Epoch 324, Validation Loss: 714.7258\n",
      "Epoch 325, Validation Loss: 714.0622\n",
      "Epoch 326, Validation Loss: 713.4018\n",
      "Epoch 327, Validation Loss: 712.7526\n",
      "Epoch 328, Validation Loss: 712.0867\n",
      "Epoch 329, Validation Loss: 711.4322\n",
      "Epoch 330, Average Loss: 72.0566\n",
      "Epoch 330, Validation Loss: 710.7853\n",
      "Epoch 331, Validation Loss: 710.1213\n",
      "Epoch 332, Validation Loss: 709.4623\n",
      "Epoch 333, Validation Loss: 708.7999\n",
      "Epoch 334, Validation Loss: 708.1326\n",
      "Epoch 335, Validation Loss: 707.4612\n",
      "Epoch 336, Validation Loss: 706.7941\n",
      "Epoch 337, Validation Loss: 706.1251\n",
      "Epoch 338, Validation Loss: 705.4587\n",
      "Epoch 339, Validation Loss: 704.7792\n",
      "Epoch 340, Average Loss: 71.2395\n",
      "Epoch 340, Validation Loss: 704.1046\n",
      "Epoch 341, Validation Loss: 703.4421\n",
      "Epoch 342, Validation Loss: 702.7961\n",
      "Epoch 343, Validation Loss: 702.1499\n",
      "Epoch 344, Validation Loss: 701.5056\n",
      "Epoch 345, Validation Loss: 700.8619\n",
      "Epoch 346, Validation Loss: 700.2326\n",
      "Epoch 347, Validation Loss: 699.6149\n",
      "Epoch 348, Validation Loss: 699.0101\n",
      "Epoch 349, Validation Loss: 698.4031\n",
      "Epoch 350, Average Loss: 70.4454\n",
      "Epoch 350, Validation Loss: 697.8118\n",
      "Epoch 351, Validation Loss: 697.2155\n",
      "Epoch 352, Validation Loss: 696.6231\n",
      "Epoch 353, Validation Loss: 696.0391\n",
      "Epoch 354, Validation Loss: 695.4712\n",
      "Epoch 355, Validation Loss: 694.8928\n",
      "Epoch 356, Validation Loss: 694.3096\n",
      "Epoch 357, Validation Loss: 693.7394\n",
      "Epoch 358, Validation Loss: 693.1696\n",
      "Epoch 359, Validation Loss: 692.6196\n",
      "Epoch 360, Average Loss: 69.6926\n",
      "Epoch 360, Validation Loss: 692.0447\n",
      "Epoch 361, Validation Loss: 691.4550\n",
      "Epoch 362, Validation Loss: 690.8620\n",
      "Epoch 363, Validation Loss: 690.2689\n",
      "Epoch 364, Validation Loss: 689.6801\n",
      "Epoch 365, Validation Loss: 689.0911\n",
      "Epoch 366, Validation Loss: 688.4907\n",
      "Epoch 367, Validation Loss: 687.8735\n",
      "Epoch 368, Validation Loss: 687.2244\n",
      "Epoch 369, Validation Loss: 686.5487\n",
      "Epoch 370, Average Loss: 68.9639\n",
      "Epoch 370, Validation Loss: 685.8709\n",
      "Epoch 371, Validation Loss: 685.1953\n",
      "Epoch 372, Validation Loss: 684.5335\n",
      "Epoch 373, Validation Loss: 683.8693\n",
      "Epoch 374, Validation Loss: 683.1995\n",
      "Epoch 375, Validation Loss: 682.5327\n",
      "Epoch 376, Validation Loss: 681.8789\n",
      "Epoch 377, Validation Loss: 681.2324\n",
      "Epoch 378, Validation Loss: 680.5861\n",
      "Epoch 379, Validation Loss: 679.9435\n",
      "Epoch 380, Average Loss: 68.2310\n",
      "Epoch 380, Validation Loss: 679.3228\n",
      "Epoch 381, Validation Loss: 678.6934\n",
      "Epoch 382, Validation Loss: 678.0392\n",
      "Epoch 383, Validation Loss: 677.3635\n",
      "Epoch 384, Validation Loss: 676.7040\n",
      "Epoch 385, Validation Loss: 676.0602\n",
      "Epoch 386, Validation Loss: 675.4304\n",
      "Epoch 387, Validation Loss: 674.8072\n",
      "Epoch 388, Validation Loss: 674.2020\n",
      "Epoch 389, Validation Loss: 673.5894\n",
      "Epoch 390, Average Loss: 67.5105\n",
      "Epoch 390, Validation Loss: 672.9656\n",
      "Epoch 391, Validation Loss: 672.3391\n",
      "Epoch 392, Validation Loss: 671.6958\n",
      "Epoch 393, Validation Loss: 671.0610\n",
      "Epoch 394, Validation Loss: 670.4418\n",
      "Epoch 395, Validation Loss: 669.8475\n",
      "Epoch 396, Validation Loss: 669.2590\n",
      "Epoch 397, Validation Loss: 668.6551\n",
      "Epoch 398, Validation Loss: 668.0394\n",
      "Epoch 399, Validation Loss: 667.4176\n",
      "Epoch 400, Average Loss: 66.8052\n",
      "Epoch 400, Validation Loss: 666.8102\n",
      "Epoch 401, Validation Loss: 666.2104\n",
      "Epoch 402, Validation Loss: 665.6106\n",
      "Epoch 403, Validation Loss: 665.0032\n",
      "Epoch 404, Validation Loss: 664.3805\n",
      "Epoch 405, Validation Loss: 663.7670\n",
      "Epoch 406, Validation Loss: 663.1581\n",
      "Epoch 407, Validation Loss: 662.5673\n",
      "Epoch 408, Validation Loss: 662.0019\n",
      "Epoch 409, Validation Loss: 661.4642\n",
      "Epoch 410, Average Loss: 66.1149\n",
      "Epoch 410, Validation Loss: 660.9299\n",
      "Epoch 411, Validation Loss: 660.3820\n",
      "Epoch 412, Validation Loss: 659.8150\n",
      "Epoch 413, Validation Loss: 659.2436\n",
      "Epoch 414, Validation Loss: 658.6661\n",
      "Epoch 415, Validation Loss: 658.0965\n",
      "Epoch 416, Validation Loss: 657.5362\n",
      "Epoch 417, Validation Loss: 657.0040\n",
      "Epoch 418, Validation Loss: 656.4719\n",
      "Epoch 419, Validation Loss: 655.9454\n",
      "Epoch 420, Average Loss: 65.4543\n",
      "Epoch 420, Validation Loss: 655.4257\n",
      "Epoch 421, Validation Loss: 654.9052\n",
      "Epoch 422, Validation Loss: 654.3792\n",
      "Epoch 423, Validation Loss: 653.8495\n",
      "Epoch 424, Validation Loss: 653.3074\n",
      "Epoch 425, Validation Loss: 652.7535\n",
      "Epoch 426, Validation Loss: 652.2006\n",
      "Epoch 427, Validation Loss: 651.6470\n",
      "Epoch 428, Validation Loss: 651.1078\n",
      "Epoch 429, Validation Loss: 650.5721\n",
      "Epoch 430, Average Loss: 64.8067\n",
      "Epoch 430, Validation Loss: 650.0406\n",
      "Epoch 431, Validation Loss: 649.5124\n",
      "Epoch 432, Validation Loss: 648.9909\n",
      "Epoch 433, Validation Loss: 648.4555\n",
      "Epoch 434, Validation Loss: 647.9084\n",
      "Epoch 435, Validation Loss: 647.3593\n",
      "Epoch 436, Validation Loss: 646.8134\n",
      "Epoch 437, Validation Loss: 646.2736\n",
      "Epoch 438, Validation Loss: 645.7367\n",
      "Epoch 439, Validation Loss: 645.2117\n",
      "Epoch 440, Average Loss: 64.1346\n",
      "Epoch 440, Validation Loss: 644.6946\n",
      "Epoch 441, Validation Loss: 644.1594\n",
      "Epoch 442, Validation Loss: 643.6358\n",
      "Epoch 443, Validation Loss: 643.1144\n",
      "Epoch 444, Validation Loss: 642.5672\n",
      "Epoch 445, Validation Loss: 641.9901\n",
      "Epoch 446, Validation Loss: 641.4221\n",
      "Epoch 447, Validation Loss: 640.8619\n",
      "Epoch 448, Validation Loss: 640.3192\n",
      "Epoch 449, Validation Loss: 639.7701\n",
      "Epoch 450, Average Loss: 63.4572\n",
      "Epoch 450, Validation Loss: 639.2064\n",
      "Epoch 451, Validation Loss: 638.6454\n",
      "Epoch 452, Validation Loss: 638.0955\n",
      "Epoch 453, Validation Loss: 637.5480\n",
      "Epoch 454, Validation Loss: 636.9780\n",
      "Epoch 455, Validation Loss: 636.3973\n",
      "Epoch 456, Validation Loss: 635.8351\n",
      "Epoch 457, Validation Loss: 635.2899\n",
      "Epoch 458, Validation Loss: 634.7581\n",
      "Epoch 459, Validation Loss: 634.2297\n",
      "Epoch 460, Average Loss: 62.7610\n",
      "Epoch 460, Validation Loss: 633.6900\n",
      "Epoch 461, Validation Loss: 633.1423\n",
      "Epoch 462, Validation Loss: 632.6111\n",
      "Epoch 463, Validation Loss: 632.0732\n",
      "Epoch 464, Validation Loss: 631.5062\n",
      "Epoch 465, Validation Loss: 630.9183\n",
      "Epoch 466, Validation Loss: 630.3200\n",
      "Epoch 467, Validation Loss: 629.7092\n",
      "Epoch 468, Validation Loss: 629.0972\n",
      "Epoch 469, Validation Loss: 628.4733\n",
      "Epoch 470, Average Loss: 62.0396\n",
      "Epoch 470, Validation Loss: 627.8392\n",
      "Epoch 471, Validation Loss: 627.2130\n",
      "Epoch 472, Validation Loss: 626.5877\n",
      "Epoch 473, Validation Loss: 625.9663\n",
      "Epoch 474, Validation Loss: 625.3606\n",
      "Epoch 475, Validation Loss: 624.7592\n",
      "Epoch 476, Validation Loss: 624.1511\n",
      "Epoch 477, Validation Loss: 623.5339\n",
      "Epoch 478, Validation Loss: 622.9074\n",
      "Epoch 479, Validation Loss: 622.2885\n",
      "Epoch 480, Average Loss: 61.3064\n",
      "Epoch 480, Validation Loss: 621.6684\n",
      "Epoch 481, Validation Loss: 621.0540\n",
      "Epoch 482, Validation Loss: 620.4268\n",
      "Epoch 483, Validation Loss: 619.7883\n",
      "Epoch 484, Validation Loss: 619.1691\n",
      "Epoch 485, Validation Loss: 618.5632\n",
      "Epoch 486, Validation Loss: 617.9516\n",
      "Epoch 487, Validation Loss: 617.3463\n",
      "Epoch 488, Validation Loss: 616.7529\n",
      "Epoch 489, Validation Loss: 616.1606\n",
      "Epoch 490, Average Loss: 60.5718\n",
      "Epoch 490, Validation Loss: 615.5504\n",
      "Epoch 491, Validation Loss: 614.8598\n",
      "Epoch 492, Validation Loss: 614.1644\n",
      "Epoch 493, Validation Loss: 613.4809\n",
      "Epoch 494, Validation Loss: 612.8120\n",
      "Epoch 495, Validation Loss: 612.1575\n",
      "Epoch 496, Validation Loss: 611.5083\n",
      "Epoch 497, Validation Loss: 610.8497\n",
      "Epoch 498, Validation Loss: 610.1792\n",
      "Epoch 499, Validation Loss: 609.4939\n",
      "Epoch 500, Average Loss: 59.8080\n",
      "Epoch 500, Validation Loss: 608.7779\n",
      "Epoch 501, Validation Loss: 608.0640\n",
      "Epoch 502, Validation Loss: 607.3694\n",
      "Epoch 503, Validation Loss: 606.6716\n",
      "Epoch 504, Validation Loss: 605.9688\n",
      "Epoch 505, Validation Loss: 605.2620\n",
      "Epoch 506, Validation Loss: 604.5774\n",
      "Epoch 507, Validation Loss: 603.9180\n",
      "Epoch 508, Validation Loss: 603.2383\n",
      "Epoch 509, Validation Loss: 602.5604\n",
      "Epoch 510, Average Loss: 59.0072\n",
      "Epoch 510, Validation Loss: 601.9024\n",
      "Epoch 511, Validation Loss: 601.2655\n",
      "Epoch 512, Validation Loss: 600.6391\n",
      "Epoch 513, Validation Loss: 600.0201\n",
      "Epoch 514, Validation Loss: 599.3837\n",
      "Epoch 515, Validation Loss: 598.7411\n",
      "Epoch 516, Validation Loss: 598.1257\n",
      "Epoch 517, Validation Loss: 597.5317\n",
      "Epoch 518, Validation Loss: 596.9359\n",
      "Epoch 519, Validation Loss: 596.3367\n",
      "Epoch 520, Average Loss: 58.2055\n",
      "Epoch 520, Validation Loss: 595.7579\n",
      "Epoch 521, Validation Loss: 595.1940\n",
      "Epoch 522, Validation Loss: 594.6413\n",
      "Epoch 523, Validation Loss: 594.0730\n",
      "Epoch 524, Validation Loss: 593.4763\n",
      "Epoch 525, Validation Loss: 592.8812\n",
      "Epoch 526, Validation Loss: 592.3146\n",
      "Epoch 527, Validation Loss: 591.7573\n",
      "Epoch 528, Validation Loss: 591.2079\n",
      "Epoch 529, Validation Loss: 590.6209\n",
      "Epoch 530, Average Loss: 57.4184\n",
      "Epoch 530, Validation Loss: 590.0180\n",
      "Epoch 531, Validation Loss: 589.4205\n",
      "Epoch 532, Validation Loss: 588.8322\n",
      "Epoch 533, Validation Loss: 588.2431\n",
      "Epoch 534, Validation Loss: 587.6410\n",
      "Epoch 535, Validation Loss: 587.0175\n",
      "Epoch 536, Validation Loss: 586.3992\n",
      "Epoch 537, Validation Loss: 585.7892\n",
      "Epoch 538, Validation Loss: 585.1994\n",
      "Epoch 539, Validation Loss: 584.5780\n",
      "Epoch 540, Average Loss: 56.6243\n",
      "Epoch 540, Validation Loss: 583.9501\n",
      "Epoch 541, Validation Loss: 583.3620\n",
      "Epoch 542, Validation Loss: 582.7905\n",
      "Epoch 543, Validation Loss: 582.2375\n",
      "Epoch 544, Validation Loss: 581.6614\n",
      "Epoch 545, Validation Loss: 581.0458\n",
      "Epoch 546, Validation Loss: 580.4015\n",
      "Epoch 547, Validation Loss: 579.7648\n",
      "Epoch 548, Validation Loss: 579.1546\n",
      "Epoch 549, Validation Loss: 578.5746\n",
      "Epoch 550, Average Loss: 55.8102\n",
      "Epoch 550, Validation Loss: 577.9832\n",
      "Epoch 551, Validation Loss: 577.3519\n",
      "Epoch 552, Validation Loss: 576.7148\n",
      "Epoch 553, Validation Loss: 576.0609\n",
      "Epoch 554, Validation Loss: 575.3895\n",
      "Epoch 555, Validation Loss: 574.6914\n",
      "Epoch 556, Validation Loss: 574.0062\n",
      "Epoch 557, Validation Loss: 573.3503\n",
      "Epoch 558, Validation Loss: 572.6776\n",
      "Epoch 559, Validation Loss: 571.9041\n",
      "Epoch 560, Average Loss: 54.9525\n",
      "Epoch 560, Validation Loss: 571.1097\n",
      "Epoch 561, Validation Loss: 570.2938\n",
      "Epoch 562, Validation Loss: 569.4279\n",
      "Epoch 563, Validation Loss: 568.5175\n",
      "Epoch 564, Validation Loss: 567.5393\n",
      "Epoch 565, Validation Loss: 566.5643\n",
      "Epoch 566, Validation Loss: 565.5979\n",
      "Epoch 567, Validation Loss: 564.6575\n",
      "Epoch 568, Validation Loss: 563.7048\n",
      "Epoch 569, Validation Loss: 562.7429\n",
      "Epoch 570, Average Loss: 53.9506\n",
      "Epoch 570, Validation Loss: 561.7887\n",
      "Epoch 571, Validation Loss: 560.8071\n",
      "Epoch 572, Validation Loss: 559.8457\n",
      "Epoch 573, Validation Loss: 558.9301\n",
      "Epoch 574, Validation Loss: 558.0679\n",
      "Epoch 575, Validation Loss: 557.2224\n",
      "Epoch 576, Validation Loss: 556.4150\n",
      "Epoch 577, Validation Loss: 555.6357\n",
      "Epoch 578, Validation Loss: 554.8379\n",
      "Epoch 579, Validation Loss: 554.0316\n",
      "Epoch 580, Average Loss: 52.9040\n",
      "Epoch 580, Validation Loss: 553.2396\n",
      "Epoch 581, Validation Loss: 552.4542\n",
      "Epoch 582, Validation Loss: 551.6705\n",
      "Epoch 583, Validation Loss: 550.8865\n",
      "Epoch 584, Validation Loss: 550.0607\n",
      "Epoch 585, Validation Loss: 549.2263\n",
      "Epoch 586, Validation Loss: 548.3784\n",
      "Epoch 587, Validation Loss: 547.4962\n",
      "Epoch 588, Validation Loss: 546.6140\n",
      "Epoch 589, Validation Loss: 545.7236\n",
      "Epoch 590, Average Loss: 51.8873\n",
      "Epoch 590, Validation Loss: 544.8691\n",
      "Epoch 591, Validation Loss: 544.0398\n",
      "Epoch 592, Validation Loss: 543.2182\n",
      "Epoch 593, Validation Loss: 542.3475\n",
      "Epoch 594, Validation Loss: 541.4984\n",
      "Epoch 595, Validation Loss: 540.6360\n",
      "Epoch 596, Validation Loss: 539.8241\n",
      "Epoch 597, Validation Loss: 539.0455\n",
      "Epoch 598, Validation Loss: 538.2780\n",
      "Epoch 599, Validation Loss: 537.4614\n",
      "Epoch 600, Average Loss: 50.8835\n",
      "Epoch 600, Validation Loss: 536.6167\n",
      "Epoch 601, Validation Loss: 535.7968\n",
      "Epoch 602, Validation Loss: 534.9899\n",
      "Epoch 603, Validation Loss: 534.1948\n",
      "Epoch 604, Validation Loss: 533.4302\n",
      "Epoch 605, Validation Loss: 532.6785\n",
      "Epoch 606, Validation Loss: 531.9283\n",
      "Epoch 607, Validation Loss: 531.1775\n",
      "Epoch 608, Validation Loss: 530.4111\n",
      "Epoch 609, Validation Loss: 529.5949\n",
      "Epoch 610, Average Loss: 49.8917\n",
      "Epoch 610, Validation Loss: 528.7665\n",
      "Epoch 611, Validation Loss: 527.9453\n",
      "Epoch 612, Validation Loss: 527.1693\n",
      "Epoch 613, Validation Loss: 526.4260\n",
      "Epoch 614, Validation Loss: 525.6979\n",
      "Epoch 615, Validation Loss: 524.9559\n",
      "Epoch 616, Validation Loss: 524.2493\n",
      "Epoch 617, Validation Loss: 523.5486\n",
      "Epoch 618, Validation Loss: 522.8097\n",
      "Epoch 619, Validation Loss: 522.0731\n",
      "Epoch 620, Average Loss: 48.8915\n",
      "Epoch 620, Validation Loss: 521.3367\n",
      "Epoch 621, Validation Loss: 520.6395\n",
      "Epoch 622, Validation Loss: 519.9544\n",
      "Epoch 623, Validation Loss: 519.2485\n",
      "Epoch 624, Validation Loss: 518.5023\n",
      "Epoch 625, Validation Loss: 517.7242\n",
      "Epoch 626, Validation Loss: 516.9694\n",
      "Epoch 627, Validation Loss: 516.2411\n",
      "Epoch 628, Validation Loss: 515.5057\n",
      "Epoch 629, Validation Loss: 514.8033\n",
      "Epoch 630, Average Loss: 47.9040\n",
      "Epoch 630, Validation Loss: 514.0876\n",
      "Epoch 631, Validation Loss: 513.3537\n",
      "Epoch 632, Validation Loss: 512.6061\n",
      "Epoch 633, Validation Loss: 511.8735\n",
      "Epoch 634, Validation Loss: 511.1589\n",
      "Epoch 635, Validation Loss: 510.4340\n",
      "Epoch 636, Validation Loss: 509.6837\n",
      "Epoch 637, Validation Loss: 508.8638\n",
      "Epoch 638, Validation Loss: 508.0385\n",
      "Epoch 639, Validation Loss: 507.2440\n",
      "Epoch 640, Average Loss: 46.9177\n",
      "Epoch 640, Validation Loss: 506.4513\n",
      "Epoch 641, Validation Loss: 505.6823\n",
      "Epoch 642, Validation Loss: 504.9514\n",
      "Epoch 643, Validation Loss: 504.2086\n",
      "Epoch 644, Validation Loss: 503.4619\n",
      "Epoch 645, Validation Loss: 502.7186\n",
      "Epoch 646, Validation Loss: 501.9427\n",
      "Epoch 647, Validation Loss: 501.1663\n",
      "Epoch 648, Validation Loss: 500.4289\n",
      "Epoch 649, Validation Loss: 499.7451\n",
      "Epoch 650, Average Loss: 45.9486\n",
      "Epoch 650, Validation Loss: 499.0989\n",
      "Epoch 651, Validation Loss: 498.4607\n",
      "Epoch 652, Validation Loss: 497.7839\n",
      "Epoch 653, Validation Loss: 497.0671\n",
      "Epoch 654, Validation Loss: 496.3640\n",
      "Epoch 655, Validation Loss: 495.6957\n",
      "Epoch 656, Validation Loss: 495.0459\n",
      "Epoch 657, Validation Loss: 494.4006\n",
      "Epoch 658, Validation Loss: 493.7973\n",
      "Epoch 659, Validation Loss: 493.2262\n",
      "Epoch 660, Average Loss: 44.9883\n",
      "Epoch 660, Validation Loss: 492.6291\n",
      "Epoch 661, Validation Loss: 491.9419\n",
      "Epoch 662, Validation Loss: 491.2110\n",
      "Epoch 663, Validation Loss: 490.4970\n",
      "Epoch 664, Validation Loss: 489.7971\n",
      "Epoch 665, Validation Loss: 489.1573\n",
      "Epoch 666, Validation Loss: 488.5259\n",
      "Epoch 667, Validation Loss: 487.9174\n",
      "Epoch 668, Validation Loss: 487.2998\n",
      "Epoch 669, Validation Loss: 486.6858\n",
      "Epoch 670, Average Loss: 44.0474\n",
      "Epoch 670, Validation Loss: 486.0897\n",
      "Epoch 671, Validation Loss: 485.4904\n",
      "Epoch 672, Validation Loss: 484.8777\n",
      "Epoch 673, Validation Loss: 484.2032\n",
      "Epoch 674, Validation Loss: 483.5318\n",
      "Epoch 675, Validation Loss: 482.8837\n",
      "Epoch 676, Validation Loss: 482.2498\n",
      "Epoch 677, Validation Loss: 481.5974\n",
      "Epoch 678, Validation Loss: 480.9752\n",
      "Epoch 679, Validation Loss: 480.3922\n",
      "Epoch 680, Average Loss: 43.1370\n",
      "Epoch 680, Validation Loss: 479.8005\n",
      "Epoch 681, Validation Loss: 479.1526\n",
      "Epoch 682, Validation Loss: 478.4970\n",
      "Epoch 683, Validation Loss: 477.8384\n",
      "Epoch 684, Validation Loss: 477.2124\n",
      "Epoch 685, Validation Loss: 476.5766\n",
      "Epoch 686, Validation Loss: 475.9298\n",
      "Epoch 687, Validation Loss: 475.3036\n",
      "Epoch 688, Validation Loss: 474.6518\n",
      "Epoch 689, Validation Loss: 473.9626\n",
      "Epoch 690, Average Loss: 42.2443\n",
      "Epoch 690, Validation Loss: 473.2766\n",
      "Epoch 691, Validation Loss: 472.6073\n",
      "Epoch 692, Validation Loss: 471.9821\n",
      "Epoch 693, Validation Loss: 471.3474\n",
      "Epoch 694, Validation Loss: 470.7201\n",
      "Epoch 695, Validation Loss: 470.0864\n",
      "Epoch 696, Validation Loss: 469.4474\n",
      "Epoch 697, Validation Loss: 468.7921\n",
      "Epoch 698, Validation Loss: 468.1375\n",
      "Epoch 699, Validation Loss: 467.4855\n",
      "Epoch 700, Average Loss: 41.3727\n",
      "Epoch 700, Validation Loss: 466.8376\n",
      "Epoch 701, Validation Loss: 466.2000\n",
      "Epoch 702, Validation Loss: 465.5796\n",
      "Epoch 703, Validation Loss: 464.9107\n",
      "Epoch 704, Validation Loss: 464.1944\n",
      "Epoch 705, Validation Loss: 463.4917\n",
      "Epoch 706, Validation Loss: 462.8507\n",
      "Epoch 707, Validation Loss: 462.2087\n",
      "Epoch 708, Validation Loss: 461.5486\n",
      "Epoch 709, Validation Loss: 460.9651\n",
      "Epoch 710, Average Loss: 40.5287\n",
      "Epoch 710, Validation Loss: 460.3593\n",
      "Epoch 711, Validation Loss: 459.7253\n",
      "Epoch 712, Validation Loss: 459.0480\n",
      "Epoch 713, Validation Loss: 458.3337\n",
      "Epoch 714, Validation Loss: 457.6154\n",
      "Epoch 715, Validation Loss: 456.9482\n",
      "Epoch 716, Validation Loss: 456.2693\n",
      "Epoch 717, Validation Loss: 455.6070\n",
      "Epoch 718, Validation Loss: 454.9489\n",
      "Epoch 719, Validation Loss: 454.2819\n",
      "Epoch 720, Average Loss: 39.7087\n",
      "Epoch 720, Validation Loss: 453.6177\n",
      "Epoch 721, Validation Loss: 452.9483\n",
      "Epoch 722, Validation Loss: 452.3064\n",
      "Epoch 723, Validation Loss: 451.6898\n",
      "Epoch 724, Validation Loss: 451.0611\n",
      "Epoch 725, Validation Loss: 450.4504\n",
      "Epoch 726, Validation Loss: 449.7792\n",
      "Epoch 727, Validation Loss: 449.0823\n",
      "Epoch 728, Validation Loss: 448.3742\n",
      "Epoch 729, Validation Loss: 447.6802\n",
      "Epoch 730, Average Loss: 38.9055\n",
      "Epoch 730, Validation Loss: 447.0166\n",
      "Epoch 731, Validation Loss: 446.3317\n",
      "Epoch 732, Validation Loss: 445.6761\n",
      "Epoch 733, Validation Loss: 445.0308\n",
      "Epoch 734, Validation Loss: 444.3668\n",
      "Epoch 735, Validation Loss: 443.7325\n",
      "Epoch 736, Validation Loss: 443.0372\n",
      "Epoch 737, Validation Loss: 442.3166\n",
      "Epoch 738, Validation Loss: 441.6148\n",
      "Epoch 739, Validation Loss: 440.9362\n",
      "Epoch 740, Average Loss: 38.1208\n",
      "Epoch 740, Validation Loss: 440.3021\n",
      "Epoch 741, Validation Loss: 439.7031\n",
      "Epoch 742, Validation Loss: 439.1194\n",
      "Epoch 743, Validation Loss: 438.5188\n",
      "Epoch 744, Validation Loss: 437.9479\n",
      "Epoch 745, Validation Loss: 437.3655\n",
      "Epoch 746, Validation Loss: 436.7963\n",
      "Epoch 747, Validation Loss: 436.1955\n",
      "Epoch 748, Validation Loss: 435.5965\n",
      "Epoch 749, Validation Loss: 435.0166\n",
      "Epoch 750, Average Loss: 37.3548\n",
      "Epoch 750, Validation Loss: 434.4640\n",
      "Epoch 751, Validation Loss: 433.8659\n",
      "Epoch 752, Validation Loss: 433.2689\n",
      "Epoch 753, Validation Loss: 432.6030\n",
      "Epoch 754, Validation Loss: 431.9461\n",
      "Epoch 755, Validation Loss: 431.3132\n",
      "Epoch 756, Validation Loss: 430.7196\n",
      "Epoch 757, Validation Loss: 430.0984\n",
      "Epoch 758, Validation Loss: 429.4663\n",
      "Epoch 759, Validation Loss: 428.8105\n",
      "Epoch 760, Average Loss: 36.6090\n",
      "Epoch 760, Validation Loss: 428.1779\n",
      "Epoch 761, Validation Loss: 427.5828\n",
      "Epoch 762, Validation Loss: 427.0160\n",
      "Epoch 763, Validation Loss: 426.4151\n",
      "Epoch 764, Validation Loss: 425.7601\n",
      "Epoch 765, Validation Loss: 425.1355\n",
      "Epoch 766, Validation Loss: 424.5598\n",
      "Epoch 767, Validation Loss: 423.9843\n",
      "Epoch 768, Validation Loss: 423.4137\n",
      "Epoch 769, Validation Loss: 422.8567\n",
      "Epoch 770, Average Loss: 35.8837\n",
      "Epoch 770, Validation Loss: 422.2663\n",
      "Epoch 771, Validation Loss: 421.6958\n",
      "Epoch 772, Validation Loss: 421.1415\n",
      "Epoch 773, Validation Loss: 420.5865\n",
      "Epoch 774, Validation Loss: 420.0088\n",
      "Epoch 775, Validation Loss: 419.4203\n",
      "Epoch 776, Validation Loss: 418.8217\n",
      "Epoch 777, Validation Loss: 418.2370\n",
      "Epoch 778, Validation Loss: 417.6213\n",
      "Epoch 779, Validation Loss: 416.9818\n",
      "Epoch 780, Average Loss: 35.0775\n",
      "Epoch 780, Validation Loss: 416.3623\n",
      "Epoch 781, Validation Loss: 415.7540\n",
      "Epoch 782, Validation Loss: 415.1494\n",
      "Epoch 783, Validation Loss: 414.5244\n",
      "Epoch 784, Validation Loss: 413.8456\n",
      "Epoch 785, Validation Loss: 413.2272\n",
      "Epoch 786, Validation Loss: 412.6521\n",
      "Epoch 787, Validation Loss: 412.0961\n",
      "Epoch 788, Validation Loss: 411.5138\n",
      "Epoch 789, Validation Loss: 410.9079\n",
      "Epoch 790, Average Loss: 34.2238\n",
      "Epoch 790, Validation Loss: 410.3166\n",
      "Epoch 791, Validation Loss: 409.7161\n",
      "Epoch 792, Validation Loss: 409.1361\n",
      "Epoch 793, Validation Loss: 408.5582\n",
      "Epoch 794, Validation Loss: 408.0189\n",
      "Epoch 795, Validation Loss: 407.5245\n",
      "Epoch 796, Validation Loss: 406.9731\n",
      "Epoch 797, Validation Loss: 406.4075\n",
      "Epoch 798, Validation Loss: 405.8098\n",
      "Epoch 799, Validation Loss: 405.1980\n",
      "Epoch 800, Average Loss: 33.4242\n",
      "Epoch 800, Validation Loss: 404.6087\n",
      "Epoch 801, Validation Loss: 404.0294\n",
      "Epoch 802, Validation Loss: 403.4939\n",
      "Epoch 803, Validation Loss: 402.9362\n",
      "Epoch 804, Validation Loss: 402.2766\n",
      "Epoch 805, Validation Loss: 401.5588\n",
      "Epoch 806, Validation Loss: 400.8337\n",
      "Epoch 807, Validation Loss: 400.0634\n",
      "Epoch 808, Validation Loss: 399.2897\n",
      "Epoch 809, Validation Loss: 398.5082\n",
      "Epoch 810, Average Loss: 32.6042\n",
      "Epoch 810, Validation Loss: 397.7809\n",
      "Epoch 811, Validation Loss: 397.1078\n",
      "Epoch 812, Validation Loss: 396.4815\n",
      "Epoch 813, Validation Loss: 395.9580\n",
      "Epoch 814, Validation Loss: 395.4897\n",
      "Epoch 815, Validation Loss: 395.0391\n",
      "Epoch 816, Validation Loss: 394.5788\n",
      "Epoch 817, Validation Loss: 394.0988\n",
      "Epoch 818, Validation Loss: 393.7399\n",
      "Epoch 819, Validation Loss: 393.4445\n",
      "Epoch 820, Average Loss: 31.7763\n",
      "Epoch 820, Validation Loss: 393.1268\n",
      "Epoch 821, Validation Loss: 392.7642\n",
      "Epoch 822, Validation Loss: 392.3512\n",
      "Epoch 823, Validation Loss: 391.9428\n",
      "Epoch 824, Validation Loss: 391.5257\n",
      "Epoch 825, Validation Loss: 391.1270\n",
      "Epoch 826, Validation Loss: 390.7214\n",
      "Epoch 827, Validation Loss: 390.2789\n",
      "Epoch 828, Validation Loss: 389.7920\n",
      "Epoch 829, Validation Loss: 389.3456\n",
      "Epoch 830, Average Loss: 30.9582\n",
      "Epoch 830, Validation Loss: 388.9681\n",
      "Epoch 831, Validation Loss: 388.6388\n",
      "Epoch 832, Validation Loss: 388.3273\n",
      "Epoch 833, Validation Loss: 388.0425\n",
      "Epoch 834, Validation Loss: 387.7785\n",
      "Epoch 835, Validation Loss: 387.5307\n",
      "Epoch 836, Validation Loss: 387.3144\n",
      "Epoch 837, Validation Loss: 387.0767\n",
      "Epoch 838, Validation Loss: 386.7878\n",
      "Epoch 839, Validation Loss: 386.3921\n",
      "Epoch 840, Average Loss: 30.2663\n",
      "Epoch 840, Validation Loss: 385.9084\n",
      "Epoch 841, Validation Loss: 385.3384\n",
      "Epoch 842, Validation Loss: 384.7796\n",
      "Epoch 843, Validation Loss: 384.2415\n",
      "Epoch 844, Validation Loss: 383.7263\n",
      "Epoch 845, Validation Loss: 383.3309\n",
      "Epoch 846, Validation Loss: 383.0827\n",
      "Epoch 847, Validation Loss: 382.9854\n",
      "Epoch 848, Validation Loss: 383.0199\n",
      "Epoch 849, Validation Loss: 383.0741\n",
      "Epoch 850, Average Loss: 29.6551\n",
      "Epoch 850, Validation Loss: 383.0396\n",
      "Epoch 851, Validation Loss: 382.9103\n",
      "Epoch 852, Validation Loss: 382.6231\n",
      "Epoch 853, Validation Loss: 382.2184\n",
      "Epoch 854, Validation Loss: 381.7600\n",
      "Epoch 855, Validation Loss: 381.2883\n",
      "Epoch 856, Validation Loss: 380.8380\n",
      "Epoch 857, Validation Loss: 380.4463\n",
      "Epoch 858, Validation Loss: 380.0999\n",
      "Epoch 859, Validation Loss: 379.8705\n",
      "Epoch 860, Average Loss: 29.0962\n",
      "Epoch 860, Validation Loss: 379.7650\n",
      "Epoch 861, Validation Loss: 379.6492\n",
      "Epoch 862, Validation Loss: 379.5189\n",
      "Epoch 863, Validation Loss: 379.3371\n",
      "Epoch 864, Validation Loss: 379.2064\n",
      "Epoch 865, Validation Loss: 379.1219\n",
      "Epoch 866, Validation Loss: 379.0221\n",
      "Epoch 867, Validation Loss: 378.9004\n",
      "Epoch 868, Validation Loss: 378.7515\n",
      "Epoch 869, Validation Loss: 378.6011\n",
      "Epoch 870, Average Loss: 28.5880\n",
      "Epoch 870, Validation Loss: 378.4299\n",
      "Epoch 871, Validation Loss: 378.2930\n",
      "Epoch 872, Validation Loss: 378.1743\n",
      "Epoch 873, Validation Loss: 378.0580\n",
      "Epoch 874, Validation Loss: 377.8958\n",
      "Epoch 875, Validation Loss: 377.7584\n",
      "Epoch 876, Validation Loss: 377.6037\n",
      "Epoch 877, Validation Loss: 377.4052\n",
      "Epoch 878, Validation Loss: 377.2128\n",
      "Epoch 879, Validation Loss: 377.0311\n",
      "Epoch 880, Average Loss: 28.0966\n",
      "Epoch 880, Validation Loss: 376.8236\n",
      "Epoch 881, Validation Loss: 376.6206\n",
      "Epoch 882, Validation Loss: 376.4211\n",
      "Epoch 883, Validation Loss: 376.1915\n",
      "Epoch 884, Validation Loss: 375.9576\n",
      "Epoch 885, Validation Loss: 375.7599\n",
      "Epoch 886, Validation Loss: 375.4678\n",
      "Epoch 887, Validation Loss: 375.1567\n",
      "Epoch 888, Validation Loss: 374.8557\n",
      "Epoch 889, Validation Loss: 374.5672\n",
      "Epoch 890, Average Loss: 27.6452\n",
      "Epoch 890, Validation Loss: 374.2968\n",
      "Epoch 891, Validation Loss: 374.0705\n",
      "Epoch 892, Validation Loss: 373.9214\n",
      "Epoch 893, Validation Loss: 373.8524\n",
      "Epoch 894, Validation Loss: 373.8114\n",
      "Epoch 895, Validation Loss: 373.7415\n",
      "Epoch 896, Validation Loss: 373.6566\n",
      "Epoch 897, Validation Loss: 373.4910\n",
      "Epoch 898, Validation Loss: 373.3103\n",
      "Epoch 899, Validation Loss: 373.2071\n",
      "Epoch 900, Average Loss: 27.2454\n",
      "Epoch 900, Validation Loss: 373.1454\n",
      "Epoch 901, Validation Loss: 373.1047\n",
      "Epoch 902, Validation Loss: 373.0740\n",
      "Epoch 903, Validation Loss: 372.9899\n",
      "Epoch 904, Validation Loss: 372.8894\n",
      "Epoch 905, Validation Loss: 372.8078\n",
      "Epoch 906, Validation Loss: 372.7247\n",
      "Epoch 907, Validation Loss: 372.7019\n",
      "Epoch 908, Validation Loss: 372.7012\n",
      "Epoch 909, Validation Loss: 372.7018\n",
      "Epoch 910, Average Loss: 26.8833\n",
      "Epoch 910, Validation Loss: 372.6595\n",
      "Epoch 911, Validation Loss: 372.5377\n",
      "Epoch 912, Validation Loss: 372.4121\n",
      "Epoch 913, Validation Loss: 372.3111\n",
      "Epoch 914, Validation Loss: 372.2858\n",
      "Epoch 915, Validation Loss: 372.2314\n",
      "Epoch 916, Validation Loss: 372.2345\n",
      "Epoch 917, Validation Loss: 372.2946\n",
      "Epoch 918, Validation Loss: 372.3287\n",
      "Epoch 919, Validation Loss: 372.2955\n",
      "Epoch 920, Average Loss: 26.5441\n",
      "Epoch 920, Validation Loss: 372.2151\n",
      "Epoch 921, Validation Loss: 372.2107\n",
      "Epoch 922, Validation Loss: 372.2785\n",
      "Epoch 923, Validation Loss: 372.3450\n",
      "Epoch 924, Validation Loss: 372.3064\n",
      "Epoch 925, Validation Loss: 372.2382\n",
      "Epoch 926, Validation Loss: 372.1972\n",
      "Epoch 927, Validation Loss: 372.1399\n",
      "Epoch 928, Validation Loss: 372.1068\n",
      "Epoch 929, Validation Loss: 372.0789\n",
      "Epoch 930, Average Loss: 26.2155\n",
      "Epoch 930, Validation Loss: 372.0675\n",
      "Epoch 931, Validation Loss: 372.0434\n",
      "Epoch 932, Validation Loss: 372.0026\n",
      "Epoch 933, Validation Loss: 371.9632\n",
      "Epoch 934, Validation Loss: 371.9796\n",
      "Epoch 935, Validation Loss: 371.9979\n",
      "Epoch 936, Validation Loss: 372.0189\n",
      "Epoch 937, Validation Loss: 371.9762\n",
      "Epoch 938, Validation Loss: 371.9560\n",
      "Epoch 939, Validation Loss: 371.9093\n",
      "Epoch 940, Average Loss: 25.9063\n",
      "Epoch 940, Validation Loss: 371.7897\n",
      "Epoch 941, Validation Loss: 371.6912\n",
      "Epoch 942, Validation Loss: 371.6230\n",
      "Epoch 943, Validation Loss: 371.5589\n",
      "Epoch 944, Validation Loss: 371.5083\n",
      "Epoch 945, Validation Loss: 371.4268\n",
      "Epoch 946, Validation Loss: 371.3664\n",
      "Epoch 947, Validation Loss: 371.3184\n",
      "Epoch 948, Validation Loss: 371.2890\n",
      "Epoch 949, Validation Loss: 371.2966\n",
      "Epoch 950, Average Loss: 25.6152\n",
      "Epoch 950, Validation Loss: 371.3023\n",
      "Epoch 951, Validation Loss: 371.2772\n",
      "Epoch 952, Validation Loss: 371.2234\n",
      "Epoch 953, Validation Loss: 371.1764\n",
      "Epoch 954, Validation Loss: 371.1755\n",
      "Epoch 955, Validation Loss: 371.1953\n",
      "Epoch 956, Validation Loss: 371.2713\n",
      "Epoch 957, Validation Loss: 371.3698\n",
      "Epoch 958, Validation Loss: 371.4635\n",
      "Epoch 959, Validation Loss: 371.4946\n",
      "Epoch 960, Average Loss: 25.3343\n",
      "Epoch 960, Validation Loss: 371.4472\n",
      "Epoch 961, Validation Loss: 371.3676\n",
      "Epoch 962, Validation Loss: 371.3570\n",
      "Epoch 963, Validation Loss: 371.3875\n",
      "Epoch 964, Validation Loss: 371.4539\n",
      "Epoch 965, Validation Loss: 371.4902\n",
      "Epoch 966, Validation Loss: 371.4952\n",
      "Epoch 967, Validation Loss: 371.4629\n",
      "Epoch 968, Validation Loss: 371.4485\n",
      "Epoch 969, Validation Loss: 371.4523\n",
      "Epoch 970, Average Loss: 25.0549\n",
      "Epoch 970, Validation Loss: 371.4170\n",
      "Epoch 971, Validation Loss: 371.4200\n",
      "Epoch 972, Validation Loss: 371.3814\n",
      "Epoch 973, Validation Loss: 371.3270\n",
      "Epoch 974, Validation Loss: 371.2873\n",
      "Early stopping at epoch 974\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:44:01.927699Z",
     "start_time": "2024-11-26T09:44:01.575939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from function import calculate_metrics\n",
    "from function import metrics_to_dataframe\n",
    "\n",
    "# 加载最佳模型的状态字典\n",
    "model.load_state_dict(torch.load('gnn_best_model.pth', weights_only=True))\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 对训练集进行预测\n",
    "    out = model(train_data)\n",
    "    print(\"训练集预测结果:\")\n",
    "    print(out)\n",
    "\n",
    "    # 计算训练集的指标\n",
    "    train_metrics = calculate_metrics(train_data.y.cpu().numpy(), out.cpu().numpy())\n",
    "    print(\"训练集指标:\", train_metrics)\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    test_out = model(test_data)\n",
    "    print(\"测试集预测结果:\")\n",
    "    print(test_out)\n",
    "\n",
    "    # 计算测试集的指标\n",
    "    test_metrics = calculate_metrics(test_data.y.cpu().numpy(), test_out.cpu().numpy())\n",
    "    print(\"测试集指标:\", test_metrics)\n",
    "\n",
    "    # 保存指标到CSV文件\n",
    "    metrics_df = metrics_to_dataframe(train_data.y.cpu().numpy(), out.cpu().numpy(),\n",
    "                                      test_data.y.cpu().numpy(), test_out.cpu().numpy(), 'GNN').round(3)\n",
    "    metrics_df.to_csv('gnn_metrics.csv', index=False)\n",
    "\n",
    "    print(metrics_df)\n",
    "\n",
    "\n",
    "# 保存训练集和测试集的预测结果（包含真实值）\n",
    "train_predictions = pd.DataFrame({'Actual': train_data.y.cpu().detach().numpy().flatten(),\n",
    "                                  'Predicted': model(train_data).cpu().detach().numpy().flatten()})\n",
    "test_predictions = pd.DataFrame({'Actual': test_data.y.cpu().detach().numpy().flatten(),\n",
    "                                 'Predicted': model(test_data).cpu().detach().numpy().flatten()})\n",
    "\n",
    "train_predictions.to_csv('gnn_train_predictions.csv', index=False)\n",
    "test_predictions.to_csv('gnn_test_predictions.csv', index=False)"
   ],
   "id": "e329431250b91a5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测结果:\n",
      "tensor([[ 63.7164],\n",
      "        [ 90.8307],\n",
      "        [ 64.7803],\n",
      "        [ 75.1128],\n",
      "        [105.2536],\n",
      "        [ 90.7775],\n",
      "        [ 97.0247],\n",
      "        [154.9189],\n",
      "        [153.2601],\n",
      "        [ 75.4091],\n",
      "        [ 52.3438],\n",
      "        [ 85.0862],\n",
      "        [149.2296],\n",
      "        [ 73.7520],\n",
      "        [ 72.9298],\n",
      "        [ 44.5800],\n",
      "        [126.3635],\n",
      "        [ 40.8420],\n",
      "        [122.2118],\n",
      "        [134.8255],\n",
      "        [ 66.3147],\n",
      "        [134.2082],\n",
      "        [ 91.8988],\n",
      "        [ 41.1624],\n",
      "        [ 99.6010],\n",
      "        [ 83.2370],\n",
      "        [ 67.9936],\n",
      "        [ 52.4740],\n",
      "        [106.0063],\n",
      "        [ 49.0375],\n",
      "        [ 99.5834],\n",
      "        [ 62.6192],\n",
      "        [132.3850],\n",
      "        [ 76.5395],\n",
      "        [106.6716],\n",
      "        [ 33.1060],\n",
      "        [ 88.7143],\n",
      "        [ 90.6142],\n",
      "        [115.9885],\n",
      "        [ 74.1767],\n",
      "        [ 81.9754],\n",
      "        [ 54.8536],\n",
      "        [ 56.2571],\n",
      "        [114.4176],\n",
      "        [ 60.4809],\n",
      "        [ 57.8788],\n",
      "        [ 63.3248],\n",
      "        [140.8880],\n",
      "        [ 30.3013],\n",
      "        [130.8002],\n",
      "        [ 43.2713],\n",
      "        [ 77.0264],\n",
      "        [ 64.4792],\n",
      "        [ 94.0008],\n",
      "        [115.9488],\n",
      "        [ 75.2467],\n",
      "        [111.4225],\n",
      "        [ 92.8984],\n",
      "        [ 68.5146],\n",
      "        [ 37.0561],\n",
      "        [ 90.2351],\n",
      "        [ 61.8739],\n",
      "        [ 69.5517],\n",
      "        [ 53.7733],\n",
      "        [ 61.1266],\n",
      "        [103.9632],\n",
      "        [ 99.9683],\n",
      "        [ 68.9213],\n",
      "        [ 99.6910],\n",
      "        [ 92.2201],\n",
      "        [ 88.1593],\n",
      "        [ 30.9274],\n",
      "        [248.5867],\n",
      "        [ 64.6472],\n",
      "        [ 72.9977],\n",
      "        [107.1480],\n",
      "        [ 83.6628],\n",
      "        [ 53.4964],\n",
      "        [144.3217],\n",
      "        [115.7980],\n",
      "        [ 92.2551],\n",
      "        [193.3952],\n",
      "        [ 99.0485],\n",
      "        [ 96.8094],\n",
      "        [ 77.4325],\n",
      "        [113.5162],\n",
      "        [ 76.3344],\n",
      "        [123.3554],\n",
      "        [163.3287],\n",
      "        [124.4312],\n",
      "        [144.1338],\n",
      "        [ 35.7534],\n",
      "        [107.0038],\n",
      "        [ 57.5920],\n",
      "        [126.3645],\n",
      "        [ 83.0869],\n",
      "        [ 92.4321],\n",
      "        [ 60.8360],\n",
      "        [ 43.5537],\n",
      "        [ 92.3256],\n",
      "        [ 99.6465],\n",
      "        [ 32.4143],\n",
      "        [ 95.6883],\n",
      "        [ 86.5169],\n",
      "        [111.8656],\n",
      "        [129.7470],\n",
      "        [105.0621],\n",
      "        [ 91.1537],\n",
      "        [ 73.5788],\n",
      "        [ 89.3500],\n",
      "        [ 41.3753],\n",
      "        [149.8585],\n",
      "        [ 43.0498],\n",
      "        [ 66.8234],\n",
      "        [145.2547],\n",
      "        [ 54.5330],\n",
      "        [104.0025],\n",
      "        [177.6936],\n",
      "        [ 81.9820],\n",
      "        [110.0272],\n",
      "        [106.8440],\n",
      "        [ 46.8501],\n",
      "        [ 54.2587],\n",
      "        [ 40.9712],\n",
      "        [ 59.9141],\n",
      "        [104.5913],\n",
      "        [120.8820],\n",
      "        [108.9994],\n",
      "        [ 76.5869],\n",
      "        [ 95.9742],\n",
      "        [ 40.4768],\n",
      "        [118.7437],\n",
      "        [ 64.4983],\n",
      "        [ 70.3880],\n",
      "        [ 66.7612],\n",
      "        [103.6921],\n",
      "        [177.6048],\n",
      "        [138.7504],\n",
      "        [114.8928],\n",
      "        [155.5755],\n",
      "        [128.4388],\n",
      "        [128.2167],\n",
      "        [116.8100],\n",
      "        [ 71.9225],\n",
      "        [115.2560],\n",
      "        [ 85.6948],\n",
      "        [ 76.3364],\n",
      "        [ 97.3562],\n",
      "        [ 82.7416],\n",
      "        [100.8382],\n",
      "        [172.2836],\n",
      "        [ 77.4233],\n",
      "        [ 92.4986],\n",
      "        [ 95.5678],\n",
      "        [ 97.2618],\n",
      "        [ 28.4867],\n",
      "        [105.7419],\n",
      "        [ 84.0754],\n",
      "        [143.2197],\n",
      "        [122.6367],\n",
      "        [ 87.0656],\n",
      "        [ 71.1727],\n",
      "        [ 66.3739],\n",
      "        [ 87.9138],\n",
      "        [153.0022],\n",
      "        [156.6070],\n",
      "        [ 64.9291],\n",
      "        [109.6186],\n",
      "        [ 81.7944],\n",
      "        [107.2907],\n",
      "        [139.4349],\n",
      "        [ 76.2096],\n",
      "        [ 55.6736],\n",
      "        [104.2200],\n",
      "        [127.0446],\n",
      "        [ 58.4028],\n",
      "        [101.0298],\n",
      "        [ 72.2618],\n",
      "        [109.5372],\n",
      "        [106.4639],\n",
      "        [109.0847],\n",
      "        [131.1114],\n",
      "        [ 37.2250],\n",
      "        [ 55.8832],\n",
      "        [109.5929],\n",
      "        [ 60.4789],\n",
      "        [ 61.5137],\n",
      "        [114.4155],\n",
      "        [ 52.9262],\n",
      "        [173.0679],\n",
      "        [103.3033],\n",
      "        [ 81.5376],\n",
      "        [145.4582],\n",
      "        [ 94.6143],\n",
      "        [ 86.3383],\n",
      "        [112.4910],\n",
      "        [ 41.1995],\n",
      "        [ 67.5986],\n",
      "        [ 71.3425],\n",
      "        [132.6965],\n",
      "        [162.2832],\n",
      "        [113.5908],\n",
      "        [126.7044],\n",
      "        [103.7858],\n",
      "        [ 76.2805],\n",
      "        [128.2061],\n",
      "        [ 87.3292],\n",
      "        [110.8431],\n",
      "        [113.4766],\n",
      "        [ 48.4214],\n",
      "        [ 85.3582],\n",
      "        [ 91.9901],\n",
      "        [150.9613],\n",
      "        [ 84.4239],\n",
      "        [105.0995],\n",
      "        [ 43.2324],\n",
      "        [ 51.9921],\n",
      "        [111.8524],\n",
      "        [131.8492],\n",
      "        [ 69.7233],\n",
      "        [118.0667],\n",
      "        [ 98.1881],\n",
      "        [ 69.7100],\n",
      "        [169.8246],\n",
      "        [ 50.2192],\n",
      "        [128.6311],\n",
      "        [ 81.8318],\n",
      "        [109.7131],\n",
      "        [ 57.0350],\n",
      "        [ 95.0895],\n",
      "        [147.0234],\n",
      "        [125.6303],\n",
      "        [165.0479],\n",
      "        [ 89.4020],\n",
      "        [ 88.2296],\n",
      "        [112.4052],\n",
      "        [104.1164],\n",
      "        [ 86.1156],\n",
      "        [101.9833],\n",
      "        [113.1965],\n",
      "        [ 41.7868],\n",
      "        [144.2658],\n",
      "        [ 84.1918],\n",
      "        [ 52.3520],\n",
      "        [166.4231],\n",
      "        [142.6232],\n",
      "        [126.4449],\n",
      "        [ 44.9510],\n",
      "        [ 90.0747],\n",
      "        [ 85.6426],\n",
      "        [ 47.7252],\n",
      "        [ 14.0921],\n",
      "        [ 83.7120],\n",
      "        [ 54.1231],\n",
      "        [126.6619],\n",
      "        [108.8356],\n",
      "        [ 40.2709],\n",
      "        [ 81.9981],\n",
      "        [156.0276],\n",
      "        [ 87.3606],\n",
      "        [ 71.1918],\n",
      "        [104.4435],\n",
      "        [ 65.9170],\n",
      "        [ 92.2302],\n",
      "        [145.1455],\n",
      "        [ 74.2766],\n",
      "        [ 92.8846],\n",
      "        [ 95.1361],\n",
      "        [ 54.6243],\n",
      "        [ 54.6228],\n",
      "        [167.0100],\n",
      "        [116.8008],\n",
      "        [104.7763],\n",
      "        [ 34.3813],\n",
      "        [ 86.4991],\n",
      "        [170.2704],\n",
      "        [105.7914],\n",
      "        [ 83.6569],\n",
      "        [ 23.1306],\n",
      "        [ 63.5330],\n",
      "        [ 71.2328],\n",
      "        [ 28.5472],\n",
      "        [ 95.6519],\n",
      "        [ 92.0281],\n",
      "        [ 74.0107],\n",
      "        [ 80.5046],\n",
      "        [128.3686],\n",
      "        [ 77.0586],\n",
      "        [ 50.6597],\n",
      "        [ 27.7668],\n",
      "        [ 72.4249],\n",
      "        [101.1105],\n",
      "        [ 58.8931],\n",
      "        [ 66.3932],\n",
      "        [124.9329],\n",
      "        [116.5218],\n",
      "        [ 50.6149],\n",
      "        [ 93.9189],\n",
      "        [189.0233],\n",
      "        [ 88.4584],\n",
      "        [167.1460],\n",
      "        [113.4511],\n",
      "        [101.9784],\n",
      "        [ 73.5703],\n",
      "        [101.2564],\n",
      "        [139.7723],\n",
      "        [111.0340],\n",
      "        [ 87.7841],\n",
      "        [145.9781],\n",
      "        [ 86.2333],\n",
      "        [179.1757],\n",
      "        [ 70.3491],\n",
      "        [ 82.4227],\n",
      "        [ 92.1532],\n",
      "        [104.5893],\n",
      "        [ 58.3741],\n",
      "        [ 65.4062],\n",
      "        [ 87.8918],\n",
      "        [127.7725],\n",
      "        [ 92.4868],\n",
      "        [ 96.5818],\n",
      "        [ 89.9683],\n",
      "        [115.8202],\n",
      "        [ 88.7321],\n",
      "        [106.9033],\n",
      "        [103.5451],\n",
      "        [111.7917],\n",
      "        [ 41.7052],\n",
      "        [ 65.2062],\n",
      "        [107.9512],\n",
      "        [116.9568],\n",
      "        [ 79.6742],\n",
      "        [ 87.3772],\n",
      "        [ 83.6949],\n",
      "        [ 86.1274],\n",
      "        [147.0260],\n",
      "        [ 35.5556],\n",
      "        [124.7902],\n",
      "        [ 71.4050],\n",
      "        [ 68.4943],\n",
      "        [ 81.0043],\n",
      "        [140.4217],\n",
      "        [117.7326],\n",
      "        [ 71.0792],\n",
      "        [ 79.4044],\n",
      "        [ 44.8490],\n",
      "        [ 90.9349],\n",
      "        [111.2426],\n",
      "        [ 53.0689],\n",
      "        [126.7374],\n",
      "        [107.4029],\n",
      "        [102.6751],\n",
      "        [ 85.4651],\n",
      "        [ 97.4922],\n",
      "        [129.2269],\n",
      "        [105.8631],\n",
      "        [111.9062],\n",
      "        [115.7437],\n",
      "        [148.6780],\n",
      "        [101.8446],\n",
      "        [ 37.5048],\n",
      "        [ 83.0202],\n",
      "        [ 72.6268],\n",
      "        [ 98.3287],\n",
      "        [ 62.6338],\n",
      "        [ 54.9037],\n",
      "        [ 79.8049],\n",
      "        [ 45.2166],\n",
      "        [ 92.0535],\n",
      "        [ 62.6963],\n",
      "        [ 66.2941],\n",
      "        [208.7870],\n",
      "        [ 90.6906],\n",
      "        [164.5683],\n",
      "        [130.7166],\n",
      "        [ 75.5071],\n",
      "        [ 65.6966],\n",
      "        [ 69.6023],\n",
      "        [ 66.0902],\n",
      "        [ 73.0022],\n",
      "        [175.9077],\n",
      "        [118.4636],\n",
      "        [ 74.3325],\n",
      "        [119.4323],\n",
      "        [ 45.9016],\n",
      "        [ 68.0796],\n",
      "        [ 46.3248],\n",
      "        [128.1535],\n",
      "        [ 35.8898],\n",
      "        [122.7866],\n",
      "        [118.0414],\n",
      "        [ 96.7252],\n",
      "        [ 59.8571],\n",
      "        [ 75.5498],\n",
      "        [152.3646],\n",
      "        [ 45.0185],\n",
      "        [132.2254],\n",
      "        [123.4609],\n",
      "        [126.3619],\n",
      "        [ 74.2321],\n",
      "        [109.3557],\n",
      "        [ 73.9774],\n",
      "        [ 80.0258],\n",
      "        [ 85.5089],\n",
      "        [119.2027],\n",
      "        [ 62.1093],\n",
      "        [ 75.2833],\n",
      "        [159.8905],\n",
      "        [195.5811],\n",
      "        [107.2967],\n",
      "        [ 76.4925],\n",
      "        [ 88.3668],\n",
      "        [ 76.6852],\n",
      "        [ 80.9738],\n",
      "        [ 86.0739],\n",
      "        [ 94.5832],\n",
      "        [ 44.9624],\n",
      "        [ 37.9028],\n",
      "        [ 40.2627],\n",
      "        [128.8661],\n",
      "        [ 94.3585],\n",
      "        [115.3579],\n",
      "        [117.0466],\n",
      "        [ 34.5137],\n",
      "        [ 62.9392],\n",
      "        [ 91.6102],\n",
      "        [ 83.7709],\n",
      "        [ 83.1015],\n",
      "        [ 90.4655],\n",
      "        [ 66.0840],\n",
      "        [102.1592],\n",
      "        [140.3528],\n",
      "        [150.2806],\n",
      "        [ 71.3453],\n",
      "        [ 95.1152],\n",
      "        [ 91.3800],\n",
      "        [ 36.2385],\n",
      "        [102.0508],\n",
      "        [166.0793],\n",
      "        [130.1669],\n",
      "        [ 71.6227],\n",
      "        [ 97.9063],\n",
      "        [ 91.2547],\n",
      "        [ 33.6646],\n",
      "        [ 36.4151],\n",
      "        [ 97.4846],\n",
      "        [ 79.9027],\n",
      "        [110.1348],\n",
      "        [ 57.3145],\n",
      "        [ 76.1473],\n",
      "        [ 91.6626],\n",
      "        [111.0697],\n",
      "        [142.0556],\n",
      "        [ 65.0274],\n",
      "        [ 98.6982],\n",
      "        [124.5005],\n",
      "        [ 47.7906],\n",
      "        [ 69.2852],\n",
      "        [ 92.7040],\n",
      "        [ 81.5630],\n",
      "        [150.5562],\n",
      "        [ 59.2505],\n",
      "        [132.8939],\n",
      "        [ 75.0634],\n",
      "        [ 98.0533],\n",
      "        [ 98.0595],\n",
      "        [ 79.3818],\n",
      "        [ 84.8530],\n",
      "        [134.2003],\n",
      "        [ 73.5442],\n",
      "        [134.8750],\n",
      "        [ 81.7751],\n",
      "        [108.4686],\n",
      "        [149.5759],\n",
      "        [103.7644],\n",
      "        [ 77.1069],\n",
      "        [ 50.1517],\n",
      "        [119.0350],\n",
      "        [ 93.3098],\n",
      "        [150.4518]], device='cuda:0')\n",
      "训练集指标: (0.8401447534561157, 11.170693, 13.624586164951324, 15.961052)\n",
      "测试集预测结果:\n",
      "tensor([[ 43.0344],\n",
      "        [ 56.4966],\n",
      "        [ 51.4498],\n",
      "        [ 53.7566],\n",
      "        [ 56.9725],\n",
      "        [ 41.9572],\n",
      "        [ 50.0654],\n",
      "        [120.3745],\n",
      "        [109.3098],\n",
      "        [138.5779],\n",
      "        [ 41.4810],\n",
      "        [ 35.0535],\n",
      "        [101.9977],\n",
      "        [ 40.6523],\n",
      "        [ 72.8821],\n",
      "        [ 33.1932],\n",
      "        [ 88.3539],\n",
      "        [115.5600],\n",
      "        [155.0337],\n",
      "        [115.5690],\n",
      "        [103.0617],\n",
      "        [ 80.1310],\n",
      "        [ 62.9186],\n",
      "        [143.2644],\n",
      "        [ 70.5571],\n",
      "        [122.1983],\n",
      "        [ 91.9693],\n",
      "        [ 65.1097],\n",
      "        [ 43.6923],\n",
      "        [ 73.2726],\n",
      "        [ 81.0677],\n",
      "        [ 57.2605],\n",
      "        [ 66.7746],\n",
      "        [ 85.6778],\n",
      "        [ 83.1752],\n",
      "        [102.0163],\n",
      "        [ 33.6587],\n",
      "        [ 86.6579],\n",
      "        [ 91.4963],\n",
      "        [123.6331],\n",
      "        [137.5148],\n",
      "        [ 58.9119],\n",
      "        [ 74.8899],\n",
      "        [149.3409],\n",
      "        [ 93.3644],\n",
      "        [ 87.0228],\n",
      "        [ 53.0762],\n",
      "        [ 70.9526],\n",
      "        [ 57.9963],\n",
      "        [116.4592],\n",
      "        [152.8263],\n",
      "        [124.3339],\n",
      "        [ 35.6837],\n",
      "        [105.6978],\n",
      "        [124.2925],\n",
      "        [ 37.3790],\n",
      "        [ 88.7749],\n",
      "        [ 95.5553],\n",
      "        [ 90.9192],\n",
      "        [102.3311],\n",
      "        [ 76.9795],\n",
      "        [105.1047],\n",
      "        [175.0114],\n",
      "        [112.5493],\n",
      "        [ 90.2887],\n",
      "        [ 51.9334],\n",
      "        [ 70.9374],\n",
      "        [ 99.6661],\n",
      "        [ 68.9905],\n",
      "        [170.7974],\n",
      "        [ 71.3682],\n",
      "        [ 98.2785],\n",
      "        [ 87.7442],\n",
      "        [ 76.7380],\n",
      "        [122.4565],\n",
      "        [ 91.6933],\n",
      "        [ 94.3692],\n",
      "        [ 59.6306],\n",
      "        [196.8927],\n",
      "        [134.5121],\n",
      "        [147.3154],\n",
      "        [ 38.2080],\n",
      "        [ 49.4996],\n",
      "        [ 74.9759],\n",
      "        [ 93.2072],\n",
      "        [ 84.7500],\n",
      "        [ 62.2536],\n",
      "        [128.8691],\n",
      "        [144.8986],\n",
      "        [ 89.3285],\n",
      "        [147.2639],\n",
      "        [129.6970],\n",
      "        [106.8002],\n",
      "        [102.9305],\n",
      "        [ 52.8101],\n",
      "        [152.7209],\n",
      "        [ 62.4316],\n",
      "        [ 71.0184],\n",
      "        [121.2786],\n",
      "        [120.9184],\n",
      "        [ 55.6561],\n",
      "        [106.6810],\n",
      "        [110.2666],\n",
      "        [174.9509],\n",
      "        [110.3148],\n",
      "        [172.5003],\n",
      "        [175.2192],\n",
      "        [ 94.5966],\n",
      "        [129.2293],\n",
      "        [161.7267],\n",
      "        [150.9309],\n",
      "        [ 59.4674],\n",
      "        [ 90.4935],\n",
      "        [ 55.9126],\n",
      "        [106.1581],\n",
      "        [ 50.3037],\n",
      "        [ 94.6850],\n",
      "        [ 76.5995],\n",
      "        [108.7008],\n",
      "        [108.3026]], device='cuda:0')\n",
      "测试集指标: (0.7643306851387024, 12.348375, 14.72543179988861, 19.265915)\n",
      "  model  R2_train  MAE_train  MAPE_train  RMSE_train  R2_test  MAE_test  \\\n",
      "0   GNN      0.84     11.171      13.625      15.961    0.764    12.348   \n",
      "\n",
      "   MAPE_test  RMSE_test  \n",
      "0     14.725  19.266001  \n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

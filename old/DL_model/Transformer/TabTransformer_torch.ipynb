{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 准备数据",
   "id": "483dc5fd0971e2b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:41:40.605590Z",
     "start_time": "2024-11-20T16:41:38.689252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "8874c47a571954c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:41:40.707714Z",
     "start_time": "2024-11-20T16:41:40.608753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"../../data/dataset.csv\")\n",
    "data['target_class'] = pd.qcut(data['Cs'], q=10, labels=False)\n",
    "X = data.drop(['Cs', 'target_class'], axis=1)\n",
    "y = data['Cs']\n",
    "stratify_column = data['target_class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=stratify_column)\n",
    "\n",
    "X_train_categ = X_train[:, 8]  # 第九列为类别特征\n",
    "X_train_cont = np.delete(X_train, 8, axis=1)  # 删除第九列，其他为连续特征\n",
    "\n",
    "# 将 NumPy 数组转换为 PyTorch 张量\n",
    "X_train_categ_tensor = torch.tensor(X_train_categ, dtype=torch.long)  # 类别特征需要使用长整型\n",
    "X_train_categ_tensor = X_train_categ_tensor.unsqueeze(1).to(device)  # 在最后一个维度添加1\n",
    "X_train_cont_tensor = torch.tensor(X_train_cont, dtype=torch.float).to(device)  # 连续特征使用浮点型\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float)  # 对于回归问题，通常使用浮点数\n",
    "y_train_tensor = y_train_tensor.unsqueeze(1).to(device)\n",
    "\n",
    "# 计算连续特征的均值和标准差\n",
    "mean = X_train_cont_tensor.mean(dim=0)\n",
    "std = X_train_cont_tensor.std(dim=0)\n",
    "continuous_mean_std = torch.stack([mean, std], dim=1).to(device)\n",
    "\n",
    "# 处理测试集\n",
    "X_test_categ = X_test[:, 8]\n",
    "X_test_cont = np.delete(X_test, 8, axis=1)\n",
    "X_test_categ_tensor = torch.tensor(X_test_categ, dtype=torch.long)\n",
    "X_test_categ_tensor = X_test_categ_tensor.unsqueeze(1).to(device)\n",
    "X_test_cont_tensor = torch.tensor(X_test_cont, dtype=torch.float).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float)\n",
    "y_test_tensor = y_test_tensor.unsqueeze(1).to(device)"
   ],
   "id": "3499816898be9093",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 定义模型",
   "id": "53feb800f58be3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:41:41.682202Z",
     "start_time": "2024-11-20T16:41:40.899049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "from torch_function import MAPE_Loss\n",
    "\n",
    "# 我们有12个特征，其中有1个类别特征，11个连续值特征\n",
    "# 类别特征每个有2个唯一值\n",
    "categories = (2,)\n",
    "num_continuous = 11\n",
    "\n",
    "# 初始化 TabTransformer 模型\n",
    "model = TabTransformer(\n",
    "    categories=categories,\n",
    "    num_continuous=num_continuous,\n",
    "    dim=16,  # 默认维度为32\n",
    "    dim_out=1,  # 回归问题的输出维度为1\n",
    "    depth=6,  # 默认深度为6\n",
    "    heads=8,  # 注意力机制的头数\n",
    "    attn_dropout=0.01,  # 注意力机制的dropout\n",
    "    ff_dropout=0.01,  # 前馈网络的的dropout\n",
    "    mlp_hidden_mults=(1, 2, 4, 1),  # MLP隐藏层的倍数\n",
    "    mlp_act=nn.ReLU(),  # MLP的激活函数, 默认为ReLU\n",
    "    continuous_mean_std=continuous_mean_std,  # 连续值的均值和标准差\n",
    ")\n",
    "\n",
    "# 将模型移动到 GPU\n",
    "model.to(device)\n",
    "\n",
    "# 初始化损失函数\n",
    "mse_loss = nn.MSELoss()\n",
    "mape_loss = MAPE_Loss().to(device)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "f3b60050a04f7e4f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练模型",
   "id": "19f0115a86873316"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:42:15.309838Z",
     "start_time": "2024-11-20T16:41:41.736076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练循环\n",
    "num_epochs = 3000\n",
    "patience = 50  # 允许的最大连续未改进 epoch 数\n",
    "epochs_without_improvement = 0  # 连续未改进的 epoch 数\n",
    "best_loss = float('inf')\n",
    "cumulative_loss = 0.0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.zero_grad()\n",
    "    outputs = model(X_train_categ_tensor, X_train_cont_tensor)\n",
    "    loss = mape_loss(outputs, y_train_tensor)  # 使用MSE损失函数\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    cumulative_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        average_loss = cumulative_loss / 10\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "        cumulative_loss = 0.0  # 重置累积损失\n",
    "\n",
    "    # 计算验证损失\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 在 GPU 上进行预测\n",
    "        y_val_pred = model(X_train_categ_tensor, X_train_cont_tensor).to(device)\n",
    "        # 验证损失计算时，确保 y_test_tensor 也在同一个设备上\n",
    "        y_train_tensor = y_train_tensor.to(device)\n",
    "        val_loss = mape_loss(y_val_pred, y_train_tensor).item()  # 计算验证损失\n",
    "\n",
    "    # 判断验证损失是否改善\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_without_improvement = 0  # 重置计数器\n",
    "        # 保存最佳模型\n",
    "        torch.save(model.state_dict(), \"tab_transformer_best_model_hidden1241.pth\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # 如果验证损失在一定次数的 epoch 内没有改进，则停止训练\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break"
   ],
   "id": "8f283be2c8de26be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 99.77881927490235\n",
      "Epoch 20, Average Loss: 97.29062881469727\n",
      "Epoch 30, Average Loss: 82.08184814453125\n",
      "Epoch 40, Average Loss: 49.3889663696289\n",
      "Epoch 50, Average Loss: 39.86666488647461\n",
      "Epoch 60, Average Loss: 39.443864822387695\n",
      "Epoch 70, Average Loss: 38.2396900177002\n",
      "Epoch 80, Average Loss: 37.9215950012207\n",
      "Epoch 90, Average Loss: 37.78740234375\n",
      "Epoch 100, Average Loss: 37.697842025756835\n",
      "Epoch 110, Average Loss: 37.61605987548828\n",
      "Epoch 120, Average Loss: 37.54196014404297\n",
      "Epoch 130, Average Loss: 37.46637496948242\n",
      "Epoch 140, Average Loss: 37.38411674499512\n",
      "Epoch 150, Average Loss: 37.292860412597655\n",
      "Epoch 160, Average Loss: 37.19094009399414\n",
      "Epoch 170, Average Loss: 37.07507057189942\n",
      "Epoch 180, Average Loss: 36.93958587646485\n",
      "Epoch 190, Average Loss: 36.77737808227539\n",
      "Epoch 200, Average Loss: 36.58445358276367\n",
      "Epoch 210, Average Loss: 36.33125228881836\n",
      "Epoch 220, Average Loss: 36.015742492675784\n",
      "Epoch 230, Average Loss: 35.60921516418457\n",
      "Epoch 240, Average Loss: 35.05191879272461\n",
      "Epoch 250, Average Loss: 34.327105331420896\n",
      "Epoch 260, Average Loss: 33.43000679016113\n",
      "Epoch 270, Average Loss: 32.310706329345706\n",
      "Epoch 280, Average Loss: 30.84523811340332\n",
      "Epoch 290, Average Loss: 29.373645782470703\n",
      "Epoch 300, Average Loss: 28.24460105895996\n",
      "Epoch 310, Average Loss: 27.594408416748045\n",
      "Epoch 320, Average Loss: 27.233399963378908\n",
      "Epoch 330, Average Loss: 26.6121618270874\n",
      "Epoch 340, Average Loss: 26.30940647125244\n",
      "Epoch 350, Average Loss: 26.148201179504394\n",
      "Epoch 360, Average Loss: 25.9864294052124\n",
      "Epoch 370, Average Loss: 25.808684158325196\n",
      "Epoch 380, Average Loss: 25.65589847564697\n",
      "Epoch 390, Average Loss: 25.48484935760498\n",
      "Epoch 400, Average Loss: 25.307858657836913\n",
      "Epoch 410, Average Loss: 25.148326301574706\n",
      "Epoch 420, Average Loss: 24.98832721710205\n",
      "Epoch 430, Average Loss: 24.801195526123045\n",
      "Epoch 440, Average Loss: 24.56145248413086\n",
      "Epoch 450, Average Loss: 24.30463390350342\n",
      "Epoch 460, Average Loss: 24.053770065307617\n",
      "Epoch 470, Average Loss: 23.776025390625\n",
      "Epoch 480, Average Loss: 23.42718162536621\n",
      "Epoch 490, Average Loss: 23.009862899780273\n",
      "Epoch 500, Average Loss: 22.63951759338379\n",
      "Epoch 510, Average Loss: 22.294595336914064\n",
      "Epoch 520, Average Loss: 21.977029609680176\n",
      "Epoch 530, Average Loss: 21.679455375671388\n",
      "Epoch 540, Average Loss: 21.526027870178222\n",
      "Epoch 550, Average Loss: 21.30526065826416\n",
      "Epoch 560, Average Loss: 21.00690746307373\n",
      "Epoch 570, Average Loss: 20.867288208007814\n",
      "Epoch 580, Average Loss: 20.643547439575194\n",
      "Epoch 590, Average Loss: 20.652449226379396\n",
      "Epoch 600, Average Loss: 20.31199378967285\n",
      "Epoch 610, Average Loss: 19.878566551208497\n",
      "Epoch 620, Average Loss: 19.717671966552736\n",
      "Epoch 630, Average Loss: 19.448716926574708\n",
      "Epoch 640, Average Loss: 19.151150512695313\n",
      "Epoch 650, Average Loss: 18.825907516479493\n",
      "Epoch 660, Average Loss: 18.541969299316406\n",
      "Epoch 670, Average Loss: 18.31321430206299\n",
      "Epoch 680, Average Loss: 18.045820045471192\n",
      "Epoch 690, Average Loss: 17.807306671142577\n",
      "Epoch 700, Average Loss: 17.449591064453124\n",
      "Epoch 710, Average Loss: 17.193513679504395\n",
      "Epoch 720, Average Loss: 16.95655574798584\n",
      "Epoch 730, Average Loss: 16.57514343261719\n",
      "Epoch 740, Average Loss: 16.43430938720703\n",
      "Epoch 750, Average Loss: 16.136305809020996\n",
      "Epoch 760, Average Loss: 15.947619152069091\n",
      "Epoch 770, Average Loss: 15.900715255737305\n",
      "Epoch 780, Average Loss: 15.46172399520874\n",
      "Epoch 790, Average Loss: 15.313445091247559\n",
      "Epoch 800, Average Loss: 15.686330986022949\n",
      "Epoch 810, Average Loss: 15.513109970092774\n",
      "Epoch 820, Average Loss: 15.19672508239746\n",
      "Epoch 830, Average Loss: 14.934906387329102\n",
      "Epoch 840, Average Loss: 14.691176319122315\n",
      "Epoch 850, Average Loss: 14.599459457397462\n",
      "Epoch 860, Average Loss: 14.521124649047852\n",
      "Epoch 870, Average Loss: 14.301347732543945\n",
      "Epoch 880, Average Loss: 14.113859176635742\n",
      "Epoch 890, Average Loss: 14.092336845397949\n",
      "Epoch 900, Average Loss: 14.22632064819336\n",
      "Epoch 910, Average Loss: 14.023154163360596\n",
      "Epoch 920, Average Loss: 13.660860729217529\n",
      "Epoch 930, Average Loss: 13.664179706573487\n",
      "Epoch 940, Average Loss: 13.583876132965088\n",
      "Epoch 950, Average Loss: 13.507448768615722\n",
      "Epoch 960, Average Loss: 13.173919010162354\n",
      "Epoch 970, Average Loss: 12.965911388397217\n",
      "Epoch 980, Average Loss: 13.282438373565673\n",
      "Epoch 990, Average Loss: 12.910426807403564\n",
      "Epoch 1000, Average Loss: 12.679482364654541\n",
      "Epoch 1010, Average Loss: 12.66521759033203\n",
      "Epoch 1020, Average Loss: 12.71306209564209\n",
      "Epoch 1030, Average Loss: 12.582318973541259\n",
      "Epoch 1040, Average Loss: 12.376022338867188\n",
      "Epoch 1050, Average Loss: 12.217898178100587\n",
      "Epoch 1060, Average Loss: 12.216603851318359\n",
      "Epoch 1070, Average Loss: 12.145596408843994\n",
      "Epoch 1080, Average Loss: 12.090880298614502\n",
      "Epoch 1090, Average Loss: 12.328575801849365\n",
      "Epoch 1100, Average Loss: 12.524117279052735\n",
      "Epoch 1110, Average Loss: 12.036083602905274\n",
      "Epoch 1120, Average Loss: 12.005145740509032\n",
      "Epoch 1130, Average Loss: 11.945071601867676\n",
      "Epoch 1140, Average Loss: 11.8328537940979\n",
      "Epoch 1150, Average Loss: 11.793881130218505\n",
      "Epoch 1160, Average Loss: 11.678399276733398\n",
      "Epoch 1170, Average Loss: 11.617019748687744\n",
      "Epoch 1180, Average Loss: 11.458971881866455\n",
      "Epoch 1190, Average Loss: 11.604807090759277\n",
      "Epoch 1200, Average Loss: 11.38454008102417\n",
      "Epoch 1210, Average Loss: 11.362560367584228\n",
      "Epoch 1220, Average Loss: 11.355501461029053\n",
      "Epoch 1230, Average Loss: 11.489753723144531\n",
      "Epoch 1240, Average Loss: 11.439604568481446\n",
      "Epoch 1250, Average Loss: 11.324505805969238\n",
      "Epoch 1260, Average Loss: 11.184900951385497\n",
      "Epoch 1270, Average Loss: 11.280927658081055\n",
      "Epoch 1280, Average Loss: 11.313325595855712\n",
      "Epoch 1290, Average Loss: 11.284653568267823\n",
      "Epoch 1300, Average Loss: 11.413742065429688\n",
      "Epoch 1310, Average Loss: 11.215076732635499\n",
      "Epoch 1320, Average Loss: 11.083794975280762\n",
      "Epoch 1330, Average Loss: 11.17437515258789\n",
      "Epoch 1340, Average Loss: 11.174265956878662\n",
      "Epoch 1350, Average Loss: 11.07791166305542\n",
      "Epoch 1360, Average Loss: 10.974976921081543\n",
      "Epoch 1370, Average Loss: 11.064321422576905\n",
      "Epoch 1380, Average Loss: 11.12948579788208\n",
      "Epoch 1390, Average Loss: 10.99687089920044\n",
      "Epoch 1400, Average Loss: 10.873700141906738\n",
      "Epoch 1410, Average Loss: 10.988875484466552\n",
      "Epoch 1420, Average Loss: 11.019531345367431\n",
      "Epoch 1430, Average Loss: 10.969622898101807\n",
      "Epoch 1440, Average Loss: 10.830390357971192\n",
      "Epoch 1450, Average Loss: 10.870351314544678\n",
      "Epoch 1460, Average Loss: 10.8953462600708\n",
      "Epoch 1470, Average Loss: 10.703234004974366\n",
      "Epoch 1480, Average Loss: 10.738432788848877\n",
      "Epoch 1490, Average Loss: 10.76062469482422\n",
      "Epoch 1500, Average Loss: 10.886202526092529\n",
      "Epoch 1510, Average Loss: 10.752319240570069\n",
      "Epoch 1520, Average Loss: 10.644142532348633\n",
      "Epoch 1530, Average Loss: 10.718687057495117\n",
      "Epoch 1540, Average Loss: 10.84063949584961\n",
      "Epoch 1550, Average Loss: 10.736155128479004\n",
      "Epoch 1560, Average Loss: 10.597334384918213\n",
      "Epoch 1570, Average Loss: 10.537076091766357\n",
      "Epoch 1580, Average Loss: 10.456580352783202\n",
      "Epoch 1590, Average Loss: 10.434867763519287\n",
      "Epoch 1600, Average Loss: 10.461906814575196\n",
      "Epoch 1610, Average Loss: 10.401212692260742\n",
      "Epoch 1620, Average Loss: 10.361324787139893\n",
      "Epoch 1630, Average Loss: 10.45115385055542\n",
      "Epoch 1640, Average Loss: 10.370597457885742\n",
      "Epoch 1650, Average Loss: 10.249957370758057\n",
      "Epoch 1660, Average Loss: 10.367371654510498\n",
      "Epoch 1670, Average Loss: 10.321160316467285\n",
      "Epoch 1680, Average Loss: 10.228990268707275\n",
      "Epoch 1690, Average Loss: 10.296797370910644\n",
      "Epoch 1700, Average Loss: 10.196018695831299\n",
      "Epoch 1710, Average Loss: 10.262731266021728\n",
      "Epoch 1720, Average Loss: 10.245502281188966\n",
      "Epoch 1730, Average Loss: 10.209239864349366\n",
      "Epoch 1740, Average Loss: 10.189332199096679\n",
      "Epoch 1750, Average Loss: 10.188451480865478\n",
      "Epoch 1760, Average Loss: 10.223887825012207\n",
      "Epoch 1770, Average Loss: 10.197868919372558\n",
      "Epoch 1780, Average Loss: 10.152516078948974\n",
      "Epoch 1790, Average Loss: 10.032803153991699\n",
      "Epoch 1800, Average Loss: 9.946574020385743\n",
      "Epoch 1810, Average Loss: 9.893227672576904\n",
      "Epoch 1820, Average Loss: 10.035420799255371\n",
      "Epoch 1830, Average Loss: 10.101358032226562\n",
      "Epoch 1840, Average Loss: 9.951989364624023\n",
      "Epoch 1850, Average Loss: 9.929457855224609\n",
      "Epoch 1860, Average Loss: 9.896161746978759\n",
      "Epoch 1870, Average Loss: 9.77181167602539\n",
      "Epoch 1880, Average Loss: 9.789150142669678\n",
      "Epoch 1890, Average Loss: 9.96129550933838\n",
      "Epoch 1900, Average Loss: 9.908156967163086\n",
      "Epoch 1910, Average Loss: 9.854185104370117\n",
      "Epoch 1920, Average Loss: 9.931538200378418\n",
      "Epoch 1930, Average Loss: 9.904947185516358\n",
      "Epoch 1940, Average Loss: 9.78602466583252\n",
      "Epoch 1950, Average Loss: 9.650478267669678\n",
      "Epoch 1960, Average Loss: 9.619731426239014\n",
      "Epoch 1970, Average Loss: 9.485625267028809\n",
      "Epoch 1980, Average Loss: 9.577781391143798\n",
      "Epoch 1990, Average Loss: 9.694464492797852\n",
      "Epoch 2000, Average Loss: 9.59243688583374\n",
      "Epoch 2010, Average Loss: 9.477459621429443\n",
      "Epoch 2020, Average Loss: 9.581307506561279\n",
      "Epoch 2030, Average Loss: 9.654762554168702\n",
      "Epoch 2040, Average Loss: 9.383994483947754\n",
      "Epoch 2050, Average Loss: 9.333615684509278\n",
      "Epoch 2060, Average Loss: 9.338271427154542\n",
      "Epoch 2070, Average Loss: 9.290969276428223\n",
      "Epoch 2080, Average Loss: 9.289096450805664\n",
      "Epoch 2090, Average Loss: 9.141769981384277\n",
      "Epoch 2100, Average Loss: 9.047213172912597\n",
      "Epoch 2110, Average Loss: 9.067586135864257\n",
      "Epoch 2120, Average Loss: 8.972941780090332\n",
      "Epoch 2130, Average Loss: 9.170466899871826\n",
      "Epoch 2140, Average Loss: 9.008498191833496\n",
      "Epoch 2150, Average Loss: 8.869110584259033\n",
      "Epoch 2160, Average Loss: 8.987024116516114\n",
      "Epoch 2170, Average Loss: 8.981847381591797\n",
      "Epoch 2180, Average Loss: 9.109493446350097\n",
      "Epoch 2190, Average Loss: 9.272937202453614\n",
      "Epoch 2200, Average Loss: 9.122623729705811\n",
      "Epoch 2210, Average Loss: 8.871180725097656\n",
      "Epoch 2220, Average Loss: 8.644309616088867\n",
      "Epoch 2230, Average Loss: 8.519705009460449\n",
      "Epoch 2240, Average Loss: 8.575418663024902\n",
      "Epoch 2250, Average Loss: 8.513556575775146\n",
      "Epoch 2260, Average Loss: 8.52095069885254\n",
      "Epoch 2270, Average Loss: 8.541908931732177\n",
      "Epoch 2280, Average Loss: 8.506915378570557\n",
      "Epoch 2290, Average Loss: 8.625186729431153\n",
      "Epoch 2300, Average Loss: 8.540898895263672\n",
      "Epoch 2310, Average Loss: 8.447828769683838\n",
      "Epoch 2320, Average Loss: 8.39054250717163\n",
      "Epoch 2330, Average Loss: 8.556830501556396\n",
      "Epoch 2340, Average Loss: 8.428048133850098\n",
      "Epoch 2350, Average Loss: 8.245280075073243\n",
      "Epoch 2360, Average Loss: 8.28507833480835\n",
      "Epoch 2370, Average Loss: 8.337949848175048\n",
      "Epoch 2380, Average Loss: 8.135120391845703\n",
      "Epoch 2390, Average Loss: 8.033667421340942\n",
      "Epoch 2400, Average Loss: 8.065812730789185\n",
      "Epoch 2410, Average Loss: 8.014991426467896\n",
      "Epoch 2420, Average Loss: 7.9610734462738035\n",
      "Epoch 2430, Average Loss: 7.99813437461853\n",
      "Epoch 2440, Average Loss: 7.987473630905152\n",
      "Epoch 2450, Average Loss: 7.914890956878662\n",
      "Epoch 2460, Average Loss: 7.940089178085327\n",
      "Epoch 2470, Average Loss: 7.8645518779754635\n",
      "Epoch 2480, Average Loss: 7.883476972579956\n",
      "Epoch 2490, Average Loss: 7.808430576324463\n",
      "Epoch 2500, Average Loss: 7.827785158157349\n",
      "Epoch 2510, Average Loss: 7.8042871952056885\n",
      "Epoch 2520, Average Loss: 7.767052745819091\n",
      "Epoch 2530, Average Loss: 7.799994611740113\n",
      "Epoch 2540, Average Loss: 8.052162504196167\n",
      "Epoch 2550, Average Loss: 7.708907556533814\n",
      "Epoch 2560, Average Loss: 7.621870756149292\n",
      "Epoch 2570, Average Loss: 7.60596981048584\n",
      "Epoch 2580, Average Loss: 7.966970634460449\n",
      "Epoch 2590, Average Loss: 7.735178279876709\n",
      "Epoch 2600, Average Loss: 7.877742195129395\n",
      "Epoch 2610, Average Loss: 7.660351848602295\n",
      "Early stopping at epoch 2614\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:42:15.846767Z",
     "start_time": "2024-11-20T16:42:15.398907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from function import metrics_to_dataframe, calculate_metrics\n",
    "\n",
    "# 加载最佳模型的状态字典\n",
    "model.load_state_dict(torch.load(\"tab_transformer_best_model_hidden1241.pth\", weights_only=True))\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 将分类和连续数据张量转移到正确的设备\n",
    "    X_train_categ_tensor = X_train_categ_tensor.to(device)\n",
    "    X_train_cont_tensor = X_train_cont_tensor.to(device)\n",
    "    y_train_tensor = y_train_tensor.to(device)\n",
    "\n",
    "    # 对训练集进行预测\n",
    "    predictions = model(X_train_categ_tensor, X_train_cont_tensor)\n",
    "    print(\"训练集预测结果:\")\n",
    "    print(predictions)\n",
    "\n",
    "    # 计算训练集的指标\n",
    "    train_metrics = calculate_metrics(y_train_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
    "    print(\"训练集指标:\", train_metrics)\n",
    "\n",
    "    # 准备测试数据\n",
    "    X_test_categ_tensor = X_test_categ_tensor.to(device)\n",
    "    X_test_cont_tensor = X_test_cont_tensor.to(device)\n",
    "    y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    test_predictions = model(X_test_categ_tensor, X_test_cont_tensor)\n",
    "    test_metrics = calculate_metrics(y_test_tensor.cpu().numpy(), test_predictions.cpu().numpy())\n",
    "    print(\"测试集指标:\", test_metrics)\n",
    "\n",
    "    # 将结果转换为DataFrame\n",
    "    tab_transformer_metrics = metrics_to_dataframe(\n",
    "        y_train_tensor.cpu().numpy(), predictions.cpu().numpy(),\n",
    "        y_test_tensor.cpu().numpy(), test_predictions.cpu().numpy(), \"TabTransformer\").round(3)\n",
    "    tab_transformer_metrics.to_csv('TabTransformer_metrics.csv', index=False)\n",
    "    print(tab_transformer_metrics)"
   ],
   "id": "caf816a60d26e411",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测结果:\n",
      "tensor([[ 45.5804],\n",
      "        [161.3616],\n",
      "        [ 67.4584],\n",
      "        [ 36.3404],\n",
      "        [ 89.0337],\n",
      "        [164.1905],\n",
      "        [ 91.0588],\n",
      "        [140.3313],\n",
      "        [167.8633],\n",
      "        [ 79.4018],\n",
      "        [ 53.4141],\n",
      "        [ 93.6236],\n",
      "        [136.8783],\n",
      "        [ 83.1137],\n",
      "        [ 88.1674],\n",
      "        [ 48.8588],\n",
      "        [110.2969],\n",
      "        [ 45.0965],\n",
      "        [114.7164],\n",
      "        [122.4980],\n",
      "        [ 66.9032],\n",
      "        [138.4997],\n",
      "        [ 79.6047],\n",
      "        [ 41.1877],\n",
      "        [101.4846],\n",
      "        [ 81.3487],\n",
      "        [ 61.2667],\n",
      "        [ 35.5792],\n",
      "        [102.2727],\n",
      "        [ 53.9852],\n",
      "        [ 95.3738],\n",
      "        [ 61.1105],\n",
      "        [112.6848],\n",
      "        [ 69.1454],\n",
      "        [101.8436],\n",
      "        [ 23.8743],\n",
      "        [ 81.1476],\n",
      "        [ 84.3423],\n",
      "        [114.3826],\n",
      "        [ 93.9023],\n",
      "        [ 73.4503],\n",
      "        [ 38.3463],\n",
      "        [ 42.9218],\n",
      "        [103.8231],\n",
      "        [ 52.6197],\n",
      "        [ 57.5152],\n",
      "        [ 54.3011],\n",
      "        [126.7592],\n",
      "        [ 14.6406],\n",
      "        [131.9092],\n",
      "        [ 34.3316],\n",
      "        [ 74.1738],\n",
      "        [ 63.4925],\n",
      "        [ 86.6104],\n",
      "        [107.6590],\n",
      "        [ 76.3367],\n",
      "        [109.0594],\n",
      "        [ 77.7782],\n",
      "        [ 48.3218],\n",
      "        [ 71.4208],\n",
      "        [ 89.0421],\n",
      "        [ 58.9952],\n",
      "        [ 72.0920],\n",
      "        [ 56.0648],\n",
      "        [ 38.9497],\n",
      "        [ 90.8956],\n",
      "        [101.8729],\n",
      "        [ 92.1614],\n",
      "        [ 74.5661],\n",
      "        [ 79.9368],\n",
      "        [ 81.4582],\n",
      "        [ 29.0533],\n",
      "        [201.6538],\n",
      "        [ 42.8500],\n",
      "        [ 67.4711],\n",
      "        [101.6562],\n",
      "        [ 77.6202],\n",
      "        [ 30.5679],\n",
      "        [148.7191],\n",
      "        [104.3253],\n",
      "        [ 83.2393],\n",
      "        [217.2600],\n",
      "        [ 91.0694],\n",
      "        [ 85.5286],\n",
      "        [ 60.6091],\n",
      "        [ 94.0032],\n",
      "        [ 72.8096],\n",
      "        [111.1061],\n",
      "        [162.5441],\n",
      "        [129.5570],\n",
      "        [143.6686],\n",
      "        [ 38.6078],\n",
      "        [103.3510],\n",
      "        [ 62.8236],\n",
      "        [118.1823],\n",
      "        [ 70.0812],\n",
      "        [ 74.0596],\n",
      "        [ 56.6245],\n",
      "        [ 25.1901],\n",
      "        [ 89.1698],\n",
      "        [100.6425],\n",
      "        [ 41.0323],\n",
      "        [103.0648],\n",
      "        [ 80.0456],\n",
      "        [ 98.2446],\n",
      "        [130.2270],\n",
      "        [108.7866],\n",
      "        [ 84.9682],\n",
      "        [ 68.7878],\n",
      "        [ 77.7790],\n",
      "        [ 45.6752],\n",
      "        [150.4701],\n",
      "        [ 21.4830],\n",
      "        [ 51.0089],\n",
      "        [136.2460],\n",
      "        [ 60.2409],\n",
      "        [ 84.5711],\n",
      "        [181.5871],\n",
      "        [ 72.6603],\n",
      "        [109.5721],\n",
      "        [110.2307],\n",
      "        [ 74.0375],\n",
      "        [ 37.6548],\n",
      "        [ 32.9259],\n",
      "        [ 90.0599],\n",
      "        [101.3636],\n",
      "        [100.1934],\n",
      "        [ 92.9593],\n",
      "        [ 65.2869],\n",
      "        [ 85.4782],\n",
      "        [ 43.7229],\n",
      "        [ 86.6353],\n",
      "        [ 49.1955],\n",
      "        [ 60.9651],\n",
      "        [ 67.1737],\n",
      "        [103.3889],\n",
      "        [170.1529],\n",
      "        [125.1127],\n",
      "        [ 97.0970],\n",
      "        [174.0527],\n",
      "        [141.8020],\n",
      "        [141.1647],\n",
      "        [118.0693],\n",
      "        [ 69.0686],\n",
      "        [120.3762],\n",
      "        [ 73.1048],\n",
      "        [ 54.0420],\n",
      "        [ 87.5060],\n",
      "        [ 81.0338],\n",
      "        [ 97.2537],\n",
      "        [158.2726],\n",
      "        [ 63.8848],\n",
      "        [ 84.3844],\n",
      "        [ 94.7922],\n",
      "        [ 80.0355],\n",
      "        [ 30.4925],\n",
      "        [ 91.9661],\n",
      "        [ 82.2822],\n",
      "        [128.7963],\n",
      "        [127.5790],\n",
      "        [ 67.4028],\n",
      "        [ 62.0892],\n",
      "        [ 60.0823],\n",
      "        [ 84.8953],\n",
      "        [143.5139],\n",
      "        [144.4383],\n",
      "        [ 66.2161],\n",
      "        [101.6642],\n",
      "        [ 79.9527],\n",
      "        [ 78.6988],\n",
      "        [131.2134],\n",
      "        [ 68.7160],\n",
      "        [ 40.4258],\n",
      "        [ 78.8842],\n",
      "        [111.5934],\n",
      "        [ 54.7275],\n",
      "        [ 91.5247],\n",
      "        [ 64.2137],\n",
      "        [ 93.5492],\n",
      "        [110.3774],\n",
      "        [105.2061],\n",
      "        [119.2845],\n",
      "        [ 41.6949],\n",
      "        [ 59.2069],\n",
      "        [113.7065],\n",
      "        [ 61.0070],\n",
      "        [ 55.2803],\n",
      "        [111.3553],\n",
      "        [ 60.5274],\n",
      "        [169.0163],\n",
      "        [102.7214],\n",
      "        [ 70.1442],\n",
      "        [130.4698],\n",
      "        [ 99.4293],\n",
      "        [ 77.5478],\n",
      "        [ 97.3341],\n",
      "        [ 42.9625],\n",
      "        [ 67.3617],\n",
      "        [ 72.0601],\n",
      "        [120.7998],\n",
      "        [152.7246],\n",
      "        [108.7338],\n",
      "        [112.6632],\n",
      "        [ 86.8974],\n",
      "        [ 77.1647],\n",
      "        [117.8370],\n",
      "        [ 79.6267],\n",
      "        [101.5329],\n",
      "        [111.9184],\n",
      "        [ 49.3991],\n",
      "        [ 66.6960],\n",
      "        [ 90.0661],\n",
      "        [158.6131],\n",
      "        [ 73.1981],\n",
      "        [104.0432],\n",
      "        [ 41.9490],\n",
      "        [ 52.8668],\n",
      "        [ 85.8952],\n",
      "        [119.9057],\n",
      "        [ 53.8832],\n",
      "        [ 90.9025],\n",
      "        [ 93.4515],\n",
      "        [ 67.9126],\n",
      "        [157.0736],\n",
      "        [ 45.5007],\n",
      "        [129.0579],\n",
      "        [ 75.3852],\n",
      "        [ 99.7535],\n",
      "        [ 52.4106],\n",
      "        [ 81.7133],\n",
      "        [134.1608],\n",
      "        [127.3203],\n",
      "        [165.2120],\n",
      "        [ 81.8000],\n",
      "        [ 79.4534],\n",
      "        [111.2139],\n",
      "        [ 99.6154],\n",
      "        [ 73.5670],\n",
      "        [103.4283],\n",
      "        [ 99.3462],\n",
      "        [ 42.2846],\n",
      "        [130.7449],\n",
      "        [ 82.9947],\n",
      "        [ 46.5548],\n",
      "        [167.3363],\n",
      "        [142.8742],\n",
      "        [121.8638],\n",
      "        [ 55.1145],\n",
      "        [ 83.6795],\n",
      "        [ 84.9503],\n",
      "        [ 45.1013],\n",
      "        [ 23.2769],\n",
      "        [ 78.9427],\n",
      "        [ 41.6312],\n",
      "        [136.5132],\n",
      "        [ 83.1917],\n",
      "        [ 39.4594],\n",
      "        [ 63.4590],\n",
      "        [159.0665],\n",
      "        [ 84.9743],\n",
      "        [ 58.0819],\n",
      "        [102.4808],\n",
      "        [ 71.5372],\n",
      "        [ 86.2193],\n",
      "        [146.5320],\n",
      "        [ 66.2871],\n",
      "        [ 83.5106],\n",
      "        [ 95.2796],\n",
      "        [ 44.9913],\n",
      "        [ 49.5376],\n",
      "        [172.8035],\n",
      "        [ 94.3343],\n",
      "        [ 89.8031],\n",
      "        [ 26.4244],\n",
      "        [ 77.7928],\n",
      "        [174.1922],\n",
      "        [108.9205],\n",
      "        [ 91.0109],\n",
      "        [ 17.5848],\n",
      "        [ 60.6105],\n",
      "        [ 71.8584],\n",
      "        [ 30.5577],\n",
      "        [ 78.0453],\n",
      "        [ 92.4792],\n",
      "        [ 71.3340],\n",
      "        [ 68.0074],\n",
      "        [114.4327],\n",
      "        [ 75.8354],\n",
      "        [ 85.0180],\n",
      "        [ 27.3092],\n",
      "        [ 93.2966],\n",
      "        [103.6563],\n",
      "        [ 66.5963],\n",
      "        [ 59.6540],\n",
      "        [106.8336],\n",
      "        [ 84.6516],\n",
      "        [ 70.1171],\n",
      "        [ 92.5551],\n",
      "        [213.6724],\n",
      "        [ 88.4858],\n",
      "        [153.6429],\n",
      "        [ 87.0388],\n",
      "        [ 92.4766],\n",
      "        [ 65.2760],\n",
      "        [ 97.8794],\n",
      "        [144.2125],\n",
      "        [105.5714],\n",
      "        [ 99.0530],\n",
      "        [141.9815],\n",
      "        [ 76.1601],\n",
      "        [172.7403],\n",
      "        [ 72.5147],\n",
      "        [ 72.5485],\n",
      "        [ 92.9521],\n",
      "        [ 95.1283],\n",
      "        [ 67.3763],\n",
      "        [ 64.9993],\n",
      "        [ 85.5424],\n",
      "        [139.8892],\n",
      "        [ 91.8029],\n",
      "        [ 97.1141],\n",
      "        [ 90.1531],\n",
      "        [121.6525],\n",
      "        [ 84.5123],\n",
      "        [ 92.4034],\n",
      "        [ 47.5939],\n",
      "        [110.1843],\n",
      "        [ 60.9045],\n",
      "        [ 66.8443],\n",
      "        [113.9138],\n",
      "        [111.6772],\n",
      "        [ 77.8365],\n",
      "        [ 80.0122],\n",
      "        [ 69.0344],\n",
      "        [ 85.1961],\n",
      "        [152.2247],\n",
      "        [ 38.1929],\n",
      "        [120.9522],\n",
      "        [ 62.8854],\n",
      "        [ 67.3629],\n",
      "        [ 69.7552],\n",
      "        [126.7008],\n",
      "        [112.6905],\n",
      "        [ 68.1042],\n",
      "        [ 68.5745],\n",
      "        [ 32.1599],\n",
      "        [ 78.6064],\n",
      "        [101.4287],\n",
      "        [ 36.2712],\n",
      "        [118.0473],\n",
      "        [103.7810],\n",
      "        [ 76.6251],\n",
      "        [ 79.7251],\n",
      "        [ 92.6441],\n",
      "        [123.0131],\n",
      "        [ 99.3850],\n",
      "        [ 81.6850],\n",
      "        [111.6580],\n",
      "        [136.2002],\n",
      "        [118.7432],\n",
      "        [ 30.1892],\n",
      "        [ 78.9857],\n",
      "        [ 56.9496],\n",
      "        [ 93.6057],\n",
      "        [ 61.6586],\n",
      "        [ 59.0719],\n",
      "        [ 80.8036],\n",
      "        [ 26.1575],\n",
      "        [ 85.2553],\n",
      "        [ 60.8051],\n",
      "        [ 75.0385],\n",
      "        [207.3377],\n",
      "        [ 88.5730],\n",
      "        [169.9492],\n",
      "        [126.0112],\n",
      "        [ 82.4827],\n",
      "        [ 66.6932],\n",
      "        [ 68.4496],\n",
      "        [ 79.5075],\n",
      "        [ 60.8511],\n",
      "        [202.6271],\n",
      "        [115.4912],\n",
      "        [ 75.5092],\n",
      "        [106.2222],\n",
      "        [ 47.5414],\n",
      "        [ 47.7120],\n",
      "        [ 36.5223],\n",
      "        [121.2031],\n",
      "        [ 21.6135],\n",
      "        [112.1274],\n",
      "        [109.9864],\n",
      "        [ 79.3723],\n",
      "        [ 58.4536],\n",
      "        [ 67.8567],\n",
      "        [142.4529],\n",
      "        [ 45.2645],\n",
      "        [134.8318],\n",
      "        [116.0173],\n",
      "        [122.3827],\n",
      "        [ 69.7938],\n",
      "        [114.6736],\n",
      "        [ 63.0241],\n",
      "        [ 69.8764],\n",
      "        [ 89.4398],\n",
      "        [117.3888],\n",
      "        [ 59.8027],\n",
      "        [ 71.3022],\n",
      "        [157.1683],\n",
      "        [219.0359],\n",
      "        [106.3762],\n",
      "        [ 63.4945],\n",
      "        [ 81.9157],\n",
      "        [ 74.8850],\n",
      "        [ 77.9712],\n",
      "        [ 86.4879],\n",
      "        [ 87.7268],\n",
      "        [ 44.5424],\n",
      "        [ 19.7093],\n",
      "        [ 43.5729],\n",
      "        [114.2491],\n",
      "        [ 76.6072],\n",
      "        [115.0424],\n",
      "        [124.6807],\n",
      "        [ 24.2184],\n",
      "        [ 41.6846],\n",
      "        [ 90.9437],\n",
      "        [ 74.7465],\n",
      "        [ 68.0883],\n",
      "        [ 80.3119],\n",
      "        [ 64.6198],\n",
      "        [ 86.0103],\n",
      "        [138.0855],\n",
      "        [160.7636],\n",
      "        [ 70.0047],\n",
      "        [ 77.3815],\n",
      "        [ 79.0789],\n",
      "        [ 38.3172],\n",
      "        [ 97.5395],\n",
      "        [166.8061],\n",
      "        [115.1975],\n",
      "        [ 68.5954],\n",
      "        [ 97.9802],\n",
      "        [ 82.1647],\n",
      "        [ 16.6455],\n",
      "        [ 28.5609],\n",
      "        [ 82.0084],\n",
      "        [ 78.0834],\n",
      "        [108.4744],\n",
      "        [ 36.2524],\n",
      "        [ 78.6635],\n",
      "        [ 92.7652],\n",
      "        [ 96.2661],\n",
      "        [141.6794],\n",
      "        [ 55.3808],\n",
      "        [ 89.8968],\n",
      "        [124.2887],\n",
      "        [ 50.7239],\n",
      "        [ 68.1590],\n",
      "        [ 86.0721],\n",
      "        [ 80.5862],\n",
      "        [157.6427],\n",
      "        [ 59.6005],\n",
      "        [127.8161],\n",
      "        [ 59.3940],\n",
      "        [ 94.8278],\n",
      "        [ 75.3334],\n",
      "        [ 76.1928],\n",
      "        [ 79.7985],\n",
      "        [128.8959],\n",
      "        [ 78.9905],\n",
      "        [131.0667],\n",
      "        [ 88.7471],\n",
      "        [ 99.9834],\n",
      "        [140.7590],\n",
      "        [105.2722],\n",
      "        [ 67.5740],\n",
      "        [ 39.0650],\n",
      "        [116.7767],\n",
      "        [ 87.9662],\n",
      "        [139.2636]], device='cuda:0')\n",
      "训练集指标: (0.8475682735443115, 8.170941, 7.4257612228393555, 15.586038)\n",
      "测试集指标: (0.8564331531524658, 8.879668, 10.80377772450447, 15.037138)\n",
      "            model  R2_train  MAE_train  MAPE_train  RMSE_train  R2_test  \\\n",
      "0  TabTransformer     0.848      8.171       7.426      15.586    0.856   \n",
      "\n",
      "   MAE_test  MAPE_test  RMSE_test  \n",
      "0      8.88     10.804     15.037  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:42:16.035355Z",
     "start_time": "2024-11-20T16:42:16.029915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存训练集和测试集的预测结果（包含真实值）\n",
    "tab_transformer_train = pd.DataFrame({'Actual': y_train_tensor.cpu().numpy().squeeze(), 'Predicted': predictions.cpu().numpy().squeeze()})\n",
    "tab_transformer_test = pd.DataFrame({'Actual': y_test_tensor.cpu().numpy().squeeze(), 'Predicted': test_predictions.cpu().numpy().squeeze()})\n",
    "\n",
    "tab_transformer_train.to_csv('tab_transformer_train.csv', index=False)\n",
    "tab_transformer_test.to_csv('tab_transformer_test.csv', index=False)"
   ],
   "id": "86de863b532e6d45",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
